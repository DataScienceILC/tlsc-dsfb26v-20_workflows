# Lesson 1 - Part 3 - Reproducible Research Tools {#represtools}

```{r setup, include=FALSE}

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE
  , 
  fig.width = 6,
  fig.height = 4
)

image_dir <- here::here(
  "images"
)

chapter_name <- "represtools"
lesson_number <- "1.3"
```


## Lesson contents

This lesson contains:

 - Open Source Software and Communities (CRAN, Bioconductor, R Open Sci)
 - Bioconductor Workflows (Docker: - How to use RStudio Docker images
 )
 - Open Peer Review (F1000)
 - RMarkdown driven development
 - `{renv}`
 - Anaconda

## Packages
```{r}
library(tidyverse)
## packages
library(ggExtra)

```

```{r, topic}
## add topics below to write them automatically to the course contents table
topics <- c(
  "Open source",
  "software development",
  "Bioconductor",
  "{renv}",
  "R Markdown",
  "R Markdown Driven Development",
  "R package development",
  "Git/Github",
  "Version Control",
  "Docker"
)

tibble(
  Lesson_Number = lesson_number,
  Lesson_Name = chapter_name,
  Topics = topics
  ) %>% 
  write_csv(
    file = "course_contents.csv", 
    append = TRUE)

```

## Resources


## Introduction
The tools that enable Open Reproducible Research are plenty. Typically they consist of programming tools that are available open source and that have user community driven development cycle.
Basically we can discriminate betwee tools that:

 1. Relate to programming languages or integrative development environments
 2. Relate to data science infrastructure & software
 3. Relate to learning tools and documentation

Below are a couple of examples for each category, of which some will be explained in this course.

1 - Data Science programming languages

```{r, dpi = 80}
knitr::include_graphics(
  file.path(
    image_dir,
    "data_science_languages",
    "InkedDia1_LI.jpg"
  )
)
```


## Tools to enable Open Science (2/3)

2 - Data Science infrastructure & software

```{r, dpi = 80}
knitr::include_graphics(
  file.path(
    image_dir,
    "data_science_languages",
    "Dia2.JPG"
  )
)
```

## Tools to enable Open Science (3/3)

3 - Data Science learning tools

```{r, dpi = 80}
knitr::include_graphics(
  file.path(
    image_dir,
    "data_science_languages",
    "Dia3.JPG"
  )
)
```


## Introducing workflows
Workflows basically are processes that help you go through your data analysis and programming flow in a reproducible and transparent manner. We already saw that when we apply the Guerrilla Analytics principle: "Do everything in code". This already in itself makes an analysis transparent, but is not yet a workflow. Workflows are definitions on how you (and your collaborators should work on code, analysis, tracking issues, resolving bugs, generating output, defining names for files). Basically everything you (as a bioinformatician or data scientist/analyst do) on a day-to-day basis. A typical workflow has the following characteristics:

 - It follows the logic that is compatible with the tool or tools that you frequently use
 - It's description is simple (the underlying structure could be complicated)
 - It uses version control in  every element and step for tracibility
 - It uses envionments for computational reproducibility


## Environments
The year is 2023:
Imagine you are programming a transcriptome analysis for one of the PhD students at the lab where you work as a programmer/bioinformatician. You are using version 5.4.6 of R and version 5.0 of Bioconductor. The experiment you are anlyzing is the first transcriptome experiment of many to come...
The year is 2026: 
You have been analyzing about a 100 transcriptome samples by now and your code lives in a github repository. You are running the analyses on the groups analysis server for computational reasons. There is a major update for R version 6.0.0 and the R kernel has been significantly overhauled. This revisons of base R has caused the reverse-dependency for EdgeR and DESeq2 to break for this R version. This means that running your existing code does not work for this new version of R. The IT department, in charge of maintenance has already updated the server to R version 6.0.0.
How do you solve this? How would you be able to still run your code. From the processes and tools used in software development, there are a number of solutions to the problem.

 1. **Projects**
 When you work with packages that need to be installed in your analysis environment it is good practice to create separate environments for each project. How you define a project can have multiple aspects. Usually a project is defined by the data being related to a single research question or research project. Using an RStudio project, linked to a git repository is a good way to start.   
 
 1. **Package environments**
 When you need (and you almost allways need) to install additional packages (R) or libraries (Python) on top of the base installation it is best ot create a package environment in your project. In this course we will show you how to do this for R. We will also provide a demo on how to do this for a complete analysis pipeline (metagenomics - `Qiime2`). We will run this Qiime2 pipline in a Docker container
 
 1. **Containers**
 Container technology has revolutionized the way we use resources for computation. Having application run in containers has the advantage that you can isolate tasks and applications from the operating system. A container is meant to run temporarily (also called ephemeral in IT jargon). It can be quicly brought up, do the thing it is supposed to and than brought down again. The absolute advantage of this is that the avaiable resources are used much more efficiently: A container will only use resources during it's life time. One it disappears, the resources it consumes are free again. We will see how we can isolate R Studio in a Docker container and than run a microbiome `Qiime2` analysis on it.
 
 1. **Version control**
 Software that can be used to track changes in code and data such as git and subversion are must-haves when you are programming and writing code. Simple: There is no other way to track changes or collaborate on code than using a repository with code under version control. In this course, you will get to know the ins and outs on using git in conjunction with github.com for working on code, managing versions, retracing your steps or fix errors and collaboration.
 
 1. **Building an R package**
 
**## EXERCISE; RStudio Projects, start with an empty or existing git repo**

 A) Start this exercise by watching these videos on RStudio resources:
  - https://rstudio.com/resources/webinars/managing-part-1-projects-in-rstudio/
  - https://rstudio.com/resources/webinars/managing-part-2-github-and-rstudio/
  
 B) We will start with an existing github repo. Clone [this github repo](https://github.com/stemangiola/bioc_2020_tidytranscriptomics/) to a new RStudio project. If you forgot how to do [that:](https://rstudio.com/resources/webinars/managing-part-1-projects-in-rstudio/)

 C) In this new project called 'bioc_2020_tidytranscriptomics', open the file "./tidytranscriptomics.Rmd", and run all the code chunks --> You will get some errors, why? What do you need to do before you are able to run this Rmd?
 
 
**## EXERCISE `{renv}` Environments for R**

Go over [this tutorial](https://rstudio.github.io/renv/articles/renv.html). Run the `{renv}` tutorial inside your 'EpiForBioWorkshop2020' project. Write down a number of clear advantages for using `{renv}`


**## EXERCISE RStudio Docker container**

 A) Install (if you have not already done so) Microsoft Visual Studio Code. You can find the software [here:](https://code.visualstudio.com/) 
 
 B) Install the Docker plugin in your Visual Studio Code. [See here](https://code.visualstudio.com/docs/containers/overview). For the Docker plugin to work, you need [Docker Desktop](https://docs.docker.com/get-docker/).
 
 C) Go over [this](https://code.visualstudio.com/docs/remote/containers-tutorial) tutorial to learn how to use Docker in Visual Studio Code
 
 D) Run the Docker container for the 'bioc_2020_tidytranscriptomics' by running the command below in your Terminal (Not the R Console!):
 
 ```
 ## run this command in your Visual Studio Code Terminal
docker run -e PASSWORD=abc -p 8787:8787 stemangiola/bioc_2020_tidytranscriptomics:bioc2020
``` 
 
 The container will be available in Visual Studio Code. You can access the container by right-clicking the container named `stemangiola/bioc_2020_tidytranscriptomics`. Choose `Open in Browser`. Alternatively, go to the website in your browser: `http://localhost:8787/`. You will get an RStudio Login screen. Username: 'rstudio' and passwd: 'abc'.
 
 D) Once you get the hang of using Docker, what benefits do you see? Write them down.
 
**EXERCISE; R package** 
In stead of building a Docker file for your project (for which you need to learn a bit more bout Docker), you can also create an R package. Usually packages are build and linked to a github repo. 

Install the R package `bioc2020tidytranscriptomics` by following the instruction in [this](https://github.com/stemangiola/bioc_2020_tidytranscriptomics) github repo. 

 A) Compare this workflow to the workflow of Docker. Write a short essay (300 words) on the advantages and disadvantages for both methods. Add why the combination of Docker and an R package in Github is even better?


## **EXERCISE; Building a simple R package**

Clone the [repo](https://github.com/UtrechtUniversity/R-data-cafe)
Open the folder /themes/start_with_rmd as a new RStudio project

Go over the file /Rmd/start_with_rmd.Rmd
If all goes well you just build your first package called 'bumblebee' with one function.

To lea







## The git/Github.com workflow; segregating data from compute infrastructure from code

The first 



```{r, dpi = 70}
knitr::include_graphics(
  file.path(image_dir,
            "git_workflow.jpg")
)
```

## Just for kicks, a graph
```{r, fig.width=8, fig.height=5}

shipman_murders <- read_csv(
  here::here(
    "data",
    "D030",
    "00-1-shipman-confirmed-victims-x.csv"
    )
  ) %>%
  janitor::clean_names()

plot <- shipman_murders %>%
ggplot(aes(
  x = fractional_death_year,
  y = age,
  colour = reorder(gender2, gender)
  )) +      
  geom_point(size = 2) +
  labs(x = "Year", y = "Age of victim") +
  scale_x_continuous(breaks = seq(1975, 1995, 5),
    limits = c(1974, 1998)) +
  scale_y_continuous(breaks = seq(40, 90, 10), limits = c(39, 95)) +
  scale_size_continuous(name = "Size", guide = FALSE) +
  scale_colour_brewer(palette = "Set1")  + 
  theme_bw() +
  theme(
    legend.position = c(0.125, 1.12),
    legend.background = element_rect(colour = "black"),
    legend.title = element_blank()) +
  ggtitle("Shipman murders")

  ggMarginal(plot, type="histogram")

```
<p style="font-size:14px">[Spiegelhalter, 2020, "The Art of Statistics"](https://github.com/dspiegel29/ArtofStatistics)</p> 

## Getting the code for all webinars into your RStudio environment; introducing the jargon {.build}

 >- **Get an Rstudio installation or account (via me)**
 >- **Clone the repo to your RStudio Env.**
 >- **Install any code dependencies in your Env.** 
 >- **Run the code, and adapt if you want**
 >- *Work on the code*
 >- *Create a commit*
 >- *Create a pull request*
 
**LIVE DEMO** 

## Github user-account

 >- https://github.com
 >- You can create personal and private repos
 >- Adding a README.md to each repo is a good idea
 >- The HU Github Data Science repos: https://github.com/uashogeschoolutrecht  
 
## RStudio

>- Integrated Development for R (and Python, Stan, C++, D3, SQL)
>- Favorite IDE for using R
>- Many integrated productivity tools (auto-completion, syntax highlighting, code-formatting, git-integrations, building tools)
>- Send me an email if you want to use R/RStudio yourself!

## Getting Github-repo content in RStudio {.build}

 >- Copied url to Github repo on clipboard
 >- Open new RStudio Project
 >- Choose 'Version Control' Option
 >- Paste url from clipboard in url field
 >- Let the clone finish
 >- Start using the code!
 >- My code will work from a cloned github repo in an RStudio Project because of the [`{here}` package!](https://github.com/jennybc/here_here) 

_Stop using `setwd()`!!!_

 **LIVE DEMO**

## HU ResearchDrive

 >- Service brought to HU by SURF
 >- Application: 
https://bibliotheek.hu.nl/onderzoekers/datamanagement/
 >- Access though webinterfacte and other software
 >- SFTP software [CyberDuck](https://cyberduck.io/) (you need admin rights)
 >- Rclone (commandline interface)

## Which tool for what?
```{r, dpi = 70}
knitr::include_graphics(
  file.path(
    image_dir,
    "Presentatie1",
    "Dia26.JPG"
  )
)
```

## Access HU-ResearchDrive from RStudio
To make this work you will need a WebDav token from HU ResearchDrive 

`Profile -> Security -> Create App`

```{r, dpi = 80}
knitr::include_graphics(
  file.path(
    image_dir,
    "Presentatie1",
    "Dia17.JPG"
  )
)
```
**Live Demo**


## Thank you for your attention!

```{r, dpi = 150}
knitr::include_graphics(
  file.path(
    image_dir,
    "Pepper.png")
)
```


