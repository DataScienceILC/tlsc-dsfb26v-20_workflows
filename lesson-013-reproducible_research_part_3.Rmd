# Lesson 1.3 - Reproducible Research Tools {#represtools}

```{r setup, include=FALSE}

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE
  , 
  fig.width = 6,
  fig.height = 4
)

image_dir <- here::here(
  "images"
)

chapter_name <- "represtools"
lesson_number <- "1.3"
```


## Lesson contents

This lesson contains:

 - Open Source Software and Communities (CRAN, Bioconductor, R Open Sci)
 - Bioconductor Workflows (Docker: - How to use RStudio Docker images
 )
 - Open Peer Review (F1000)
 - RMarkdown driven development
 - `{renv}`
 - Anaconda

## Packages
```{r}
library(tidyverse)
## packages
library(ggExtra)

```

```{r, topic}
## add topics below to write them automatically to the course contents table
topics <- c(
  "Open source",
  "software development",
  "Bioconductor",
  "{renv}",
  "R Markdown",
  "R Markdown Driven Development",
  "R package development",
  "Git/Github",
  "Version Control",
  "Docker"
)

tibble(
  Lesson_Number = lesson_number,
  Lesson_Name = chapter_name,
  Topics = topics
  ) %>% 
  write_csv(
    file = "course_contents.csv", 
    append = TRUE)

```

## Resources


## Introduction
The tools that enable Open Reproducible Research are plenty. Typically they consist of programming tools that are available open source and that have user community driven development cycle.
Basically we can discriminate betwee tools that:

 1. Relate to programming languages or integrative development environments
 2. Relate to data science infrastructure & software
 3. Relate to learning tools and documentation

Below are a couple of examples for each category, of which some will be explained in this course.

1 - Data Science programming languages

```{r, dpi = 80}
knitr::include_graphics(
  file.path(
    image_dir,
    "data_science_languages",
    "InkedDia1_LI.jpg"
  )
)
```


## Tools to enable Open Science (2/3)

2 - Data Science infrastructure & software

```{r, dpi = 80}
knitr::include_graphics(
  file.path(
    image_dir,
    "data_science_languages",
    "Dia2.jpg"
  )
)
```

## Tools to enable Open Science (3/3)

3 - Data Science learning tools

```{r, dpi = 80}
knitr::include_graphics(
  file.path(
    image_dir,
    "data_science_languages",
    "Dia3.jpg"
  )
)
```


## Introducing workflows
Workflows basically are processes that help you go through your data analysis and programming flow in a reproducible and transparent manner. We already saw that when we apply the Guerrilla Analytics principle: "Do everything in code". This already in itself makes an analysis transparent, but is not yet a workflow. Workflows are definitions on how you (and your collaborators should work on code, analysis, tracking issues, resolving bugs, generating output, defining names for files). Basically everything you (as a bioinformatician or data scientist/analyst do) on a day-to-day basis. A typical workflow has the following characteristics:

 - It follows the logic that is compatible with the tool or tools that you frequently use
 - It's description is simple (the underlying structure could be complicated)
 - It uses version control in  every element and step for tracibility
 - It uses envionments for computational reproducibility


## Environments
The year is 2023:
Imagine you are programming a transcriptome analysis for one of the PhD students at the lab where you work as a programmer/bioinformatician. You are using version 5.4.6 of R and version 5.0 of Bioconductor. The experiment you are anlyzing is the first transcriptome experiment of many to come...
The year is 2026: 
You have been analyzing about a 100 transcriptome samples by now and your code lives in a github repository. You are running the analyses on the groups analysis server for computational reasons. There is a major update for R version 6.0.0 and the R kernel has been significantly overhauled. This revisons of base R has caused the reverse-dependency for EdgeR and DESeq2 to break for this R version. This means that running your existing code does not work for this new version of R. The IT department, in charge of maintenance has already updated the server to R version 6.0.0.
How do you solve this? How would you be able to still run your code. From the processes and tools used in software development, there are a number of solutions to the problem.

 1. Projects
 When you work with packages that need to be installed in your analysis environment it is good practice to create separate environments for each project. How you define a project can have multiple aspects. Usually a project is defined by the data being related to a single research question or research project. Using an RStudio project, linked to a git repository is a good way to start.   
 1. Package environments
 When you need (and you almost allways need) to install additional packages (R) or libraries (Python) on top of the base installation it is best ot create a package environment in your project. In this course we will show you how to do this for R. We will also provide a demo on how to do this for a complete analysis pipeline (metagenomics - `Qiime2`). We will run this Qiime2 pipline in a Docker container
 1. Containers
 Container technology has revolutionized the way we use resources for computation. Having application run in containers has the advantage that you can isolate tasks and applications from the operating system. A container is meant to run temporarily (also called ephemeral in IT jargon). It can be quicly brought up, do the thing it is supposed to and than brought down again. The absolute advantage of this is that the avaiable resources are used much more efficiently: A container will only use resources during it's life time. One it disappears, the resources it consumes are free again. We will see how we can isolate R Studio in a Docker container and than run a microbiome `Qiime2` analysis on it.
 1. git/Github
 Git and Github.com
 




## EXERCISE; RStudio Projects

## EXERCISE; `{renv}`

## **EXERCISE; Docker images**


## **EXERCISE; Using `{renv}` as package manager for R**



## EXERCISE; Using `conda` as package manager for Python








## The git/Github.com workflow; segregating data from compute infrastructure from code

The first 



```{r, dpi = 70}
knitr::include_graphics(
  file.path(image_dir,
            "git_workflow.jpg")
)
```

## Just for kicks, a graph
```{r, fig.width=8, fig.height=5}

shipman_murders <- read_csv(
  here::here(
    "data",
    "D030",
    "00-1-shipman-confirmed-victims-x.csv"
    )
  ) %>%
  janitor::clean_names()

plot <- shipman_murders %>%
ggplot(aes(
  x = fractional_death_year,
  y = age,
  colour = reorder(gender2, gender)
  )) +      
  geom_point(size = 2) +
  labs(x = "Year", y = "Age of victim") +
  scale_x_continuous(breaks = seq(1975, 1995, 5),
    limits = c(1974, 1998)) +
  scale_y_continuous(breaks = seq(40, 90, 10), limits = c(39, 95)) +
  scale_size_continuous(name = "Size", guide = FALSE) +
  scale_colour_brewer(palette = "Set1")  + 
  theme_bw() +
  theme(
    legend.position = c(0.125, 1.12),
    legend.background = element_rect(colour = "black"),
    legend.title = element_blank()) +
  ggtitle("Shipman murders")

  ggMarginal(plot, type="histogram")

```
<p style="font-size:14px">[Spiegelhalter, 2020, "The Art of Statistics"](https://github.com/dspiegel29/ArtofStatistics)</p> 

## Getting the code for all webinars into your RStudio environment; introducing the jargon {.build}

 >- **Get an Rstudio installation or account (via me)**
 >- **Clone the repo to your RStudio Env.**
 >- **Install any code dependencies in your Env.** 
 >- **Run the code, and adapt if you want**
 >- *Work on the code*
 >- *Create a commit*
 >- *Create a pull request*
 
**LIVE DEMO** 

## Github user-account

 >- https://github.com
 >- You can create personal and private repos
 >- Adding a README.md to each repo is a good idea
 >- The HU Github Data Science repos: https://github.com/uashogeschoolutrecht  
 
## RStudio

>- Integrated Development for R (and Python, Stan, C++, D3, SQL)
>- Favorite IDE for using R
>- Many integrated productivity tools (auto-completion, syntax highlighting, code-formatting, git-integrations, building tools)
>- Send me an email if you want to use R/RStudio yourself!

## Getting Github-repo content in RStudio {.build}

 >- Copied url to Github repo on clipboard
 >- Open new RStudio Project
 >- Choose 'Version Control' Option
 >- Paste url from clipboard in url field
 >- Let the clone finish
 >- Start using the code!
 >- My code will work from a cloned github repo in an RStudio Project because of the [`{here}` package!](https://github.com/jennybc/here_here) 

_Stop using `setwd()`!!!_

 **LIVE DEMO**

## HU ResearchDrive

 >- Service brought to HU by SURF
 >- Application: 
https://bibliotheek.hu.nl/onderzoekers/datamanagement/
 >- Access though webinterfacte and other software
 >- SFTP software [CyberDuck](https://cyberduck.io/) (you need admin rights)
 >- Rclone (commandline interface)

## Which tool for what?
```{r, dpi = 70}
knitr::include_graphics(
  file.path(
    image_dir,
    "Presentatie1",
    "Dia26.JPG"
  )
)
```

## Access HU-ResearchDrive from RStudio
To make this work you will need a WebDav token from HU ResearchDrive 

`Profile -> Security -> Create App`

```{r, dpi = 80}
knitr::include_graphics(
  file.path(
    image_dir,
    "Presentatie1",
    "Dia17.JPG"
  )
)
```
**Live Demo**


## Thank you for your attention!

```{r, dpi = 150}
knitr::include_graphics(
  file.path(
    image_dir,
    "Pepper.png")
)
```


