# Data management {#represdata}
```{r setuples, include=FALSE}
les <- 2
```

```{r, dpi = 50, echo=FALSE}
knitr::include_graphics(
  here::here(
    "images",
    "one-does-not-simply-migrate-data.jpg"
  )
)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE, 
  fig.width = 6,
  fig.height = 4
)

image_dir <- here::here(
  "images"
)
```

```{r, include=FALSE}
## packages
library(tidyverse)
library(readxl)

```

## Aim of this lesson
After this lesson you will be familiar with the most important data management guidelines and principles: 

<mark>
 1. Proper file naming strategies
 1. Guerrilla Analytics principles
 1. Files and folders / Project structure
 1. Data-formats & Data shapes / Tidy data
 1. Preparing your data for sharing
 1. Encoding variables & Exploratory Data Analysis
 1. Annotations and meta data
 1. Column names and file names
 1. Checking data validity/integrity using checksums
 1. Proprietary vs non-proprietary formats
</mark>

## File names

**Do you recognize this?**
```{r, dpi=50, echo=FALSE}
knitr::include_graphics(
  file.path(
  image_dir,
    "final_final.png"
  )
)

```
https://medium.com/@jameshoareid/final-pdf-finalfinal-pdf-actualfinal-pdf-cae61ab1d94c

The use of version control abolishes the need for inventing a file name every time you save it. You will learn more about using version control (git and github.com) in lesson 2 and 3. With the use of git version control you only have to think about naming a file just once with a good name. But what entitles a 'good' file name?

A good file name is:

 1. Unique in a folder (prevent duplicated names)
 1. Is short, but descriptive (if you need it to be longer to be descriptive enough, choose that)
 1. Does not contain any special characters* except for `_` and a `.` before the extension. Having multiple dots (`.`) in a file name can be confusing but sometimes is required. For example for an archive we sometimes see `<file_name>.tar.gz`
 1. The typeface of a file name is ideally set in lowercase only. If you want to deviate from this use `UpperFirst` camelcase instead.
 1. The most important thing about naming files is to be consistent. This is also the hardest part!
 1. If you receive a file from somebody else: **Never change the file name, even if it does not meet the above requirements**. Changing  a file name causes a breakage between the file and the source it came from. If you change a file name you recieved from a person or downloaded from the internet, the person who send the file will not know about the new name.

*The special characters you should avoid in a file name:
```
! @ # $ % ^ & * ( ) + - = : " ' ; ? > < ~ / ? { } [ ] \ | ` , 
```

Special characters are reserved for other purposes and can cause problems when a back-up of the files is made or when files need to be loaded in analyzing software or when copying files.

**Basically, what was stated about file names, also applied to naming variables in a dataset. Or, with other words: choosing or creating valid names for columns in a data frame, or names of R objects for that matter.

```{r, dpi = 50, echo=FALSE}
knitr::include_graphics(
  here::here(
    "images",
    "bad-characters.png"
    
  )
)
```

Below, I show an example of badly formatted file name and column names to make the point.
```{r, dpi = 80, echo=FALSE}
knitr::include_graphics(
  file.path(image_dir, "bad_formatting_file_name_and_headers.jpg")
)
```

_what is wrong with this file name and its headers? can you spot another problem with the data sheet?_

## The Guerrilla Analytics Principles
To help you build a thorough data management process for yourself that you can start using and expanding when needed, we need a framework. In this course we use the `Guerrilla Analytics` framework, as described by Enda Ridge in [this booklet](https://www.elsevier.com/books/T/A/9780128002186). If you are pursuing a career in Data Science, I highly recommend getting a copy! This booklet describes in a very practical, and hands-on way, how to establish a data management workflow. Either when you work all by yourself or in a team, the pointers in this book will be applicable in both situations. Once you get to know this framework, you will discover that you used to do it wrong (like me...). 

To build the framework, let's look ath the core 7 Guerrilla Analytics Principles:

 >1. Space is cheap, confusion is expensive
 >1. Use simple, visual project structures and conventions
 >3. Automate (_everything - my adittion_) with program code
 >4. Link  stored data to data in the analytics environment to data in work products (_literate programming with RMarkdown - my addition_)
 >5. Version control changes to data and analytics code (Git/Github.com)
 >6. Consolidate team knowledge (agree on guidelines and stick to it as a team)  
 >7. Use code that runs from start to finish (_literate programming with RMarkdown - my addition_)

<p style="font-size:14px">[Guerrilla Analytics book by Enda Ridge, ](https://guerrilla-analytics.net/)</p>

```{r, dpi = 300, echo=FALSE}
knitr::include_graphics(
  here::here("images", "guerrilla_analytics.jpg")
)
```

As you can see from my own edits to these principles, quite a few are immediately applicable when using a programming Language like R and its Rmd format, that we use all the time. I will go over each principle below in more detail. But first an exercise.

<div class="question">
##### Exercise `r les` {-}
Imagine you receive a file attached to an email from a researcher in your research group called: 

`salmonella CFU kinetics OD600 in LB van ipecs 8okt2020 kleur.xlsx`

The file is located in the course `./data` folder

You are requested to run an analysis on the raw data in this file (sheet named 'All Cycles'). It contains data from a plate reader experiment where wells are measured over time. The researcher asks you to generate a plot per sample. No other information was provided in the original email. 

Describe the following steps in an RMarkdown file. You do not need to write the R code for the actual analysis at this point - we will do that later in another exercise in this lesson. Answer these questions in your markdown file with exercises.

 1. How would you prepare for this analysis in R/Rstudio?
 1. Look at the contents of the file, which Excel Worksheet do you need for the analysis? 
 1. Which steps do you need to take to load the data into R
 1. Which steps do you need to take in reshaping the file to make the dataformat match the requirement for using `{ggplot}
 1. Think of a better file name
 1. Write a conceptual and kind and friendly, but clear reply email (in English) to the researcher, where you address the changes that the researcher needs to make to the file in order for you to be able to analyze this data in R.
 1. Add the resulting RMarkdown to your RStudio project portfolio project for this course (see prerequisites), you will need to ad this file to your bookdown portfolio later in lesson \@ref(bookdownportfolio)
 
</div> 
 
Now that we encountered a data management challenge in the Wild, let's build our framework to be able to tackle these types of problems in a more structured fashion, next time we meet them.  
 
## The Guerrilla Principles explained  

### Principle 1: Space is cheap, confusion is expensive
This principle is simple: storage costs are low these days so there is no need to spend a lot of time on administrating files. 

 1. Keep your files, you never know when you need them. The price for a storage units has dramatically dropped over the past years, so there is no need to delete files or clean up any more. Just save old files in an archive. 
 1. Store data that you actively work on in online-cloud storage. Usually, you will be better of storing data on a remote location in the cloud. This reduces the risk of data loss, when something happens to the local file system. Cloud storage is usually redundant. Which means copies of the same file are ditributed over mupltiple locations (even geographically if you want). When one disk fails, there is always a back up, so no real data loss if hardware dies.
 1. Protect yourself: do not click on attachments and spiffy emails, cyber criminals are getting smarter everyday. 
 <!-- When installing open source software always check the checksums of the downloaded file. More on this topic later in @\ref(checksums) -->
 1. Create md5sums for important (source) data-files to ensure data integrity and data transfer validity. A MD5 ‘hash’ or ‘checksum’ is a 128-bit summary of the file contents. Files with different MD5 sums are different. So you can use this to check whether a file has changed since you last used it.
 1. Agree on a storage system, share it, use it, stick to it
 
```{r, echo=FALSE}
knitr::include_graphics(
  file.path(
    image_dir,
    "docking_station_harddisk.jpg"
  )
)
```

MD5sumss are unique codes to identify a file. In the following example, we find the MD5sum for a .txt file in /data/md5_examples:

```{r examplemd5}
library(tools)
md5_ex1_raw <- tools::md5sum(
  here::here(
    "data",
    "md5_examples",
    "MD5_exampledata_1.txt"
  )
)
```

Use enframe() to get atomic vectors or lists nice and tidy in a tibble:
```{r}
md5_ex1_raw %>% enframe() -> md5sums_ex1

```

<div class="question">
##### Exercise `r les` {-}

There are 3 more datafiles in the folder /data/md5_examples. Find out which one is different from the other three using their MD5sums.
If you want, `md5sum` can handle multiple files at the same time, but wants full paths for all of them.

</div>

<details><summary>Click for the answer</summary>
```{r}
#library(tools)

myDir <- here::here(
    "data",
    "md5_examples")

fileNames <- list.files(myDir, recursive = TRUE)

tools::md5sum(file.path(myDir, fileNames)) %>% enframe() -> md5sums_all
md5sums_all$filename <- fileNames
md5sums_all %>% select(filename,value)

```
</details>



### Principle 2: Simple, visual project structures and conventions    
This principle concerns the _organisation_ of files and folders on the file system. Because you usually do not work alone, but in a team, this is an important point to agree upon within your team. If you do not stick to the rules you agreed on playing by, you will find that playing the game is no fun at all. When organizing a classical project structure, people with no data science background tend to organize files and folders on the contextual, or categorical level. This means you would have something like this:

```{r, echo=FALSE}
knitr::include_graphics(
  here::here(
    "images",
    "project_structure_categorical.png"
  )
)
```

This makes sense but has at least two major drawbacks: 

 1. Deep nesting of files in folder causes the absolute path to a file to grow. The path length on some file systems (especially in the cloud like OneDrive or GoogleDrive have a limit to the length of a path size. When copying a file or folder, this may cause the file or folder not be copied. Usually this error is silent. Meaning, you will not know about it, before it is too late and you discover years later that a complete folder is empty - trust me, it happens, I've been there!)
 1. Categories and projects evolve. So at the time of putting the structure in place this may seem like a logical structure, but things will change. There will be new categories and new types of files, you had not thought of before. Where will these go?

See for my own bad example on how not to do it:
```{r}
fs::dir_tree(here::here("wrong_structure"))
```

So when managing files in projects adhere to the following guidelines

 1. Create a separate folder for each analytics project (in RStudio -> RStudio Project). Keep the unit of a project small. So a larger research project can usually be subdivided into smaller projects. Create an analysis project for each sub-project. Don't try to fit it all together.
 1. Do not deeply nest folders (max 2-3 levels)
 1. Keep information about the data, close to the data. So store descriptions about variables for example in a README.txt file and store that README.txt file in the same folder as where the dat lives that it describes. When using Excel files, you may choose to store the README information in a separate worksheet. More on this later \@ref(metadata1)
 1. Store each dataset in its own sub-folder. I usually work like the example below. Each dataset I receive goes in a numbered folder in the `data-raw` folder. If I recieve an updated file, I add the old file to a new folder called `v01` inside the orginal `D..` folder. The updated file replaces the `v01` file. If then you recieve yet another update, you can repeat this trick with a `v02` folder inside the `D..` folder. In this way, the file directly in the `D..` folder is always the latest updated version. The README.txt file containing data about the data (see \@ref(metadata1)) lives also in the root of the `D..` folder.
 1. Do not change file names or move them. As explained before, never change a file name of a file you recieve or download from the internet. If you need to change the file for analysis reasons (sometimes bioinformatics pipelines require specially formatted file names and than you do not have a choice, but to rename them), record your changes in the README.txt file!
 1. Do not manually edit data source files. **Never, ever change a value inside a file**. You can (_almost_) always solve this using code. 
 1. In code, use relative paths

How to organize data files
```{r}
fs::dir_tree(here::here("data-raw"))
```

#### Data integrity {-} 
<!-- {#checksums} -->
When receiving a file from a laboratory that has performed a sequencing analysis, the files you receive are usually in `.fastq.gz` or `fasta.gz` format. Because these files can be big, they are usually accompanied by a small file containing a hash-like string looking something like this:

`r tools::md5sum(here::here("data", "tidy_Kopie van salmonella CFU kinetics OD600 in LB van ipecs 8okt2020 kleur.xlsx"))`

This is the md5sums checksums for the file used earlier: `./data/salmonella CFU kinetics OD600 in LB van ipecs 8okt2020 kleur.xlsx`. When the file changes the checksums also changes, like we will see in the following exercise.

There are a number of different algorithms with which we can calculate such so-called `sumchecks`. Here we use the md5sums, which is a popular hashing algorithm.

md5sums are

 1. A unique code to identify a file
 1. Can be used to verify the integrity or the version of a file
 1. Can be genarated from Windows, MacOS, Linux or from within e.g. R/Python/Bash
 1. md5sums are also used for safety: checking an md5sum ensures that the code is valid and has not changed (e.g. [Anaconda](https://www.digitalocean.com/community/tutorials/how-to-install-anaconda-on-ubuntu-18-04-quickstart))
 1. There are many different types of hash functions [MD5, SHA256 are much used for data and software](https://en.wikipedia.org/wiki/List_of_hash_functions)
 
<div class="question">
##### Exercise `r les`{-}
Calculating and checking md5sums checksums in R

 (a) Determine the md5sums of the file. Save the checksums to a file [write a piece of R code] `./data/toxrefdb_nel_lel_noael_loael_summary_AUG2014_FOR_PUBLIC_RELEASE.csv`
 
<details><summary>Click for the answer</summary>
```{r}

## (a)
# calculate md5sums for file "toxrefdb_nel_lel_noael_loael_summary_AUG2014_FOR_PUBLIC_RELEASE.csv"
tools::md5sum(
  here::here(
    "data",
    "toxrefdb_nel_lel_noael_loael_summary_AUG2014_FOR_PUBLIC_RELEASE.csv"
  )
) %>%
  enframe() -> md5sums_toxref



md5sums_toxref %>% # write as dataframe for easy access
  readr::write_csv(
    here::here(
      "data",
      "toxrefdb_nel_lel_noael_loael_summary_AUG2014_FOR_PUBLIC_RELEASE.md5")
  )


```

</details>

 (b) Download this file to your own laptop from the RStudio Server
 
 (c) Using the command line (in Windows, use Git bash for Windows or Windos Powershell, in Mac or Linus, use the Terminal), determine the md5sums of the file you just downloaded. Again, save the resulting md5sums in a file
 
<details><summary>Click for the answer</summary>

```
# (b)
# I recommend using Git Bah for Windows [Could not make it work on Win-PowerShell]
# see: https://stackoverflow.com/questions/478722/what-is-the-best-way-to-calculate-a-checksum-for-a-file-that-is-on-my-machine
# https://stackoverflow.com/questions/10521061/how-to-get-an-md5-checksum-in-powershell
## In GitBash for Windows, this worked:

CertUtil -hashfile toxrefdb_nel_lel_noael_loael_summary_AUG2014_FOR_PUBLIC_RELEASE.csv MD5 > toxrefdb_nel_lel_noael_loael_summary_AUG2014_FOR_PUBLIC_RELEASE_LOCAL.md5

# Mac/Linux
md5sum <pathToFileToCheck> > <pathToFileToCheck.md5>

```
</div> 
 
 (d) Do the md5sums you calculated on the server and the md5sums of the local file match?

``
You should get `r md5sums_toxref$value` as md5sums on both the server and locally
 
 (e) Can you think of a way to check this in an R script? Write the script and save it in a .R file

<details><summary>Click for the answer</summary>
```{r}
# calculate checksums from local file as under (b) and write to file 
# calculate checksums from local file as under (b) and write to file 
# From RSTudio

# load both md5sum files into R
# you could just logically compare the two srings, but is there a better way?
serverside <- read_csv(
  here::here(
    "data",
    "toxrefdb_nel_lel_noael_loael_summary_AUG2014_FOR_PUBLIC_RELEASE.md5"
  )
)

local <- read_lines(
  here::here(
    "data",
    "toxrefdb_nel_lel_noael_loael_summary_AUG2014_FOR_PUBLIC_RELEASE_local.md5"
  )
) %>%
  enframe()


## check equality
serverside$value == local$value[2]

```

</details>

### Sharing data 
When you share data with someone else, consider taking these steps: 

 >- Remove sensitive data from each file by pseudoencoding or anonymizing or removing
 >- Encoding sensitive data can be done from within R. [See here ](https://stackoverflow.com/questions/21686645/how-to-create-md5-hash-of-a-column-in-r)
 >- Agree on a file naming convention within a team, before the work starts
 >- Agree on where data is stored and who has access
 >- Suppress the impulse to store multiple copies of the data in different locations
 >- If you sent data files, sent the md5sums along

### Principle 3: Automate _everything_ with program code
If we consider this lesson is about Reproducible Research, this is the core principle we need to consider thoroughly. From the R programming perspective, we can do this easily when using RMarkdown in combination with scripts. It already automates everything from the data file to the final results of an analysis. And, what is even better: it is neatly packaged in a reproducible report that, when rendered to html, can be opened on every computer.

Take heed of these pointers:

 >- Do everything programatically (in code) for reasons of reproducibility
 >- Store clean curated datasets in the "data" folder, with md5sums and a README.txt
 >- Use literate programming (RMarkdown or Jupyter Notebook) for full analysis
 >- Store scripts in a "./code" or "./inst" folder
 <!-- >- Store (R) functions in R in a "./R" folder, and document them with [Roxygen2](https://cran.r-project.org/web/packages/roxygen2/vignettes/roxygen2.html) -->

<!-- XXX heel graag, maar dan moeten we dat uitleggen in de cursus. Voor nu doorgeschoven naar volgend jaar. -->

<div class="question">
##### Exercise `r les` {-}
Let's demonstrate this principle 3 with a COVID-19 reporting example

 >- Imagine we want daily reports on the number of COVID-19 cases and caused deaths
 >- We want to be able to dynamically report data for different countries and dates to compare situations in the World
 >- The data is available (for manual and automated download) from the European Center for Disease Control
 >- The analysis can be coded completely from begin to end to result in the information we need

Take a look at the source file in this [Github repo (click)](https://raw.githubusercontent.com/uashogeschoolutrecht/covid_demo/master/app/covid_data.Rmd)

Download this file to your RStudio environment and knit the file. 

What do you see in the rendered html? 

What happens if you change som of the parameters in the `yaml` header of the file?

</div>

#### Parameterization {-}
The above example in the exercise is an example of a so-called _parameterized_ script. In this case a parameterized RMarkdown. We will learn more on parameterizing RMarkdown files in lesson \@ref(rmarkdownparams)

 >- The covid Rmd is parameterized on date and country
 >- The script automatically includes the parameters in the title of the report and the captions of the figures
 >- The 'rendered' date is automatically set, for tracking and versioning purposes
 >- Parameterization can used to automate reporting for many values of parameters
 >- Further automation is easy now (although the ECDC has regular 'changes' to their latest data available for download - and they do not use md5sums!! - This makes full automation and building--in checks more difficult)

```{r, dpi = 60, echo=FALSE}
knitr::include_graphics(here::here(
    "images",
    "covid_rmd_screenshot.jpg"
  )
)
```

### Principle 4: Link `stored data`, to data in the `analytics environment`, to data in `work products`

Basically this can be done with literate programming with R or Python in RStudio or Jupyter:

 - The data is stored on disk or in the Cloud
 - The `analytics environment` is the Global Environment (where variables and R-objects live)
 - Data is pulled from the storage in the Analytics Environment by a script  
 - The work products (Rmd / Notebooks) bring it together

```{r, dpi=200, fig.align='right'}
knitr::include_graphics(
  file.path(
  image_dir,
  "one_ring.jpg")
)
```

### Principle 5: Version control for data and code - Git/Github

 >- When you do data analysis, you should use code. See also Principle 4.
 > - When you write code, you should use Git, preferably in combination with Github. Or use another version control system.
 >- Hence: When you do data analysis, you should use Git & Github 
 >- Git/Github is 'track-changes for code'

You will learn more on using the git/github workflow in data science in lesson 3 and 4 (\@ref(gitintro), \@ref(gitrstudio), \@ref(gitbranchmerge), \@ref(gitcollaboration)). 

### Principle 6: Consolidate team knowledge 
When working together it is vital to come to an agreement on how you work together. I hope the Guerrilla Analytics framework provides a starting point. Hopefully, you will learn during your projecticum work how vital this actually is when working together on a data project or any project for that matter. Here are some pointers:  

 >- Make guidelines on datamanagement, storage places and workflows
 >- Agree within the team on them
 >- Stick to them! And be frank about it when things go wrong or people misbehave. An honest and open collaborative envrionment is encouraging. It is usually hard for people to change their way of working. 
 >- Work together in a virtual collaboration environment.
 >- Work together on code using Github or some other version control based repository system (e.g. Gitlab / Bitbucket).
 >- Provide for education and share best practices within the organization, the department and/or the team (this is what we try to achieve with this course).

### Principle 7: Prefer analytics code that runs from start to finish

 - Create work products in RMarkdown or Jupyter notebooks
 - [In R, create an R-package](https://github.com/UtrechtUniversity/R-data-cafe/tree/master/themes/start_with_rmd). Once you have a fully functional RMarkdown file, it is quite easy to rebase that code into an R Package. I call this the "Start with Rmd" - workflow. The demo in the link shows you an example.
 - Write functions that isolate code and can be recycled. When writing a function, think about how to generalize this function so that you can recycle it in other projects. This saves time and adds robustness.
 - Use iterations to prevent repetition. Write clear and compact loops, for example in R by using the map-family of functions from the `{purrr}` package. I prefer these above writing `for()` loops because they focus on the object that is being changed, not on the computation that is done.

Now that we have a framework with which we can build our work flows in a data science project we can start working and collaborating. Below I resume some key concepts that are useful when working in a data science team.  

## Data formatting
During the different R courses we have been working with data in the `tidy` format frequently. 
```{r, echo=FALSE}
knitr::include_graphics(
  file.path(image_dir, "tidy-1.png")
)
```

 1. Each variable goes in its own column
 1. Each observation goes in its own row
 1. Each cell contains only one value

<p style="font-size:14px">From: ["R for Data Science", Grolemund and Wickham](https://r4ds.had.co.nz/)</p> 

Although this is a optimized format for working with the `{tidyverse}` tools it is not the only suitable data format. We already encountered an important other structure that is much used in Bioinformatics: `SummarizedExperiment`. This class of data format is optimized for working with `Bioconductor` packages and work flows. 

```{r, echo=FALSE}
knitr::include_graphics(
  here::here(
    "images",
    "summarizedexperiment.png"
  ))
```

For machine learning purposes, data is often formatted in the `wide` format. We see an example here: 
```{r}
data(package = "mlbench", dataset = "BostonHousing")
BostonHousing %>% as_tibble()
```
The different variables are arranged in a side-by-side fashion. In this example the data is still tidy, but there are also examples of wide formatted data that is not tidy. When you want to work with this data, you first need to transform it in a stacked or so-called long format that works well for `{tidyverse}`. We will see an example in the next exercise

<div class="question">
##### Exercise `r les` {-}
Remember the well plate experiment from last lesson. Here we will perform a transformation on the data to make the data suitable for analysis in R. We will also create a single graph showing all the data available for all samples over measured time in the experiment. 

The data file for this exercise can be found here: `./data/salmonella CFU kinetics OD600 in LB van ipecs 8okt2020 kleur.xlsx`

Go over the following steps to complete this exercise. Add the resulting RMarkdown to your portfolio bookdown project (see \@ref(bookdownportfolio).

 1. Review your answers to the previous exercise where we used this file in this lesson.
 1. Try reading the sheet called `All Cycles` in the Excel file. 
 1. What goes wrong with the formatting of the data if you start reading in the data from cell A1?
 1. Try solving this problem. 
 1. What shape of formatting would you say this data is in? Is the data tidy?
 1. Write a piece of code that creates a tidy format of this data. You also need to take a look a the sheet called `layout` to get information on the samples. Try generating a manual data frame that has 96 rows and a code for each sample. The experiment has been performed in duplo, so for each experimental condition there are two samples. 
 1. Now join your `sample data` dataframe to the raw data.
 1. Export the data as a .csv file.
 1. Write an appropriate README.txt file that accompanies this exported csv file. Save both in your `data` folder of your bookdown portfolio project.
 
**TIPS:

 - Remember: `dplyr::pivot_longer()` and `dplyr::pivot_wider()` are very helpful when you want to reshape your data in R
 - After reading your data into R: be sure to check the datatype of the columns
 - Create a sample data table containing sample information for each of the 96 samples mentioned in the `All Cycles` sheet. The information you need to do this is contained in sheet `layout`
 - the `time` variable in this dataset is a nasty one. It is recorded in an uncenventional way. You need to use some cleaning up code to transform this variable in numbers (use `str_replace_all()`, to get rid of the stupid characters like `x` and the `_`. Next you can transform this variable into hours and minutes by using `seperate()`. Be aware that there will be 'empty' cells in the `minutes` column 
 - Do you think it is a good idea to have graphs in an Excel worksheet that contains the raw data?
 - How would you have stored the Raw data of this experiment?
 
</div>

<details><summary>Click for the answer</summary>
answer
```{r, cache=TRUE}
# reading in the data - without any special settings
library(readxl)

data_platereader <- read_xlsx(
  here::here(
    "data",
    "salmonella CFU kinetics OD600 in LB van ipecs 8okt2020 kleur.xlsx"
  ), sheet = "All Cycles"
)
## this data looks mangled because of several things: 
# there is some metadata in the top region of the sheet
# there is a weird looking headers (two headers?)

## trying skip
data_platereader <- read_xlsx(
  here::here(
    "data",
    "salmonella CFU kinetics OD600 in LB van ipecs 8okt2020 kleur.xlsx"
  ), sheet = "All Cycles", skip = 11
)

## clean up and fix names
data_platereader <- data_platereader %>%
  rename(sample = Time, well = ...1) %>%
  janitor::clean_names()

## which wells have data?
unique(data_platereader$well)


## create sample table
sample_names <- data_platereader$sample

mv_utr_tx100 <- rep(c("mv", "mv", "mv", "mv", 
                      "untr", "untr", "untr", "untr", "untr",
                      "tx100", "tx100", "tx100"), times = 8)

salmonella <- read_xlsx(
  here::here(
    "data",
    "salmonella CFU kinetics OD600 in LB van ipecs 8okt2020 kleur.xlsx"
  ), sheet = "layout", range = "C5:N13"
) %>%
  janitor::clean_names() 

# cheack data types
map(
  .x = salmonella,
  typeof
)

salmonella <- salmonella %>%
  pivot_longer(ul_sal_1:ul_sal_12,
               names_to = "plate_column", 
               values_to = "microliters_bacteria")

## synthesize to sample table

samples <- tibble(
  well = data_platereader$well,  
  sample = sample_names,
  condition = mv_utr_tx100,
  ul_salmonella = salmonella$microliters_bacteria
)

## join sample table with data
data_join <- left_join(samples, data_platereader)

## create tidy version
data_tidy <- data_join %>%
  pivot_longer(
    x0_h:x24_h_5_min,
    names_to = "time",
    values_to = "value"
  )

## fix time variable
data_tidy_time <- data_tidy %>%
  mutate(time_var =
  str_replace_all(
    string = time,
    pattern = "x",
    replacement = ""
  )) %>%
  mutate(time_var =
  str_replace_all(
    string = time_var,
    pattern = "_",
    replacement = ""
  )) %>%
  mutate(time_var =
  str_replace_all(
    string = time_var,
    pattern = "h",
    replacement = ":"
  )) %>%
  mutate(time_var =
  str_replace_all(
    string = time_var,
    pattern = "min",
    replacement = ""
  )) %>%
  separate(
    col = time_var,
    into = c("hours", "minutes"),
    remove = FALSE
  ) %>%
  mutate(
    minutes = ifelse(minutes == "", "0", minutes)
  ) %>%
  mutate(minutes_passed = 60*as.numeric(hours) + as.numeric(minutes))

## misingness
data_tidy %>%
  naniar::vis_miss()

## graphs
data_tidy_time %>%
  group_by(condition, ul_salmonella, minutes_passed) %>%
  summarise(mean_value = mean(value)) %>%
  mutate(ul_salmonella = round(as.numeric(ul_salmonella), 2)) %>%
  ggplot(aes(x = minutes_passed, y = mean_value)) +
  geom_line(aes(colour = condition), show.legend = FALSE) +
  facet_grid(condition ~ ul_salmonella) +
  xlab("Time passed (minutes)") +
  ylab("Mean AU")

```

</details>

## Metadata {#metadata1}
Meta information like type of variable, ranges, units, number of observations or subjects in a study, type of analysis or experimental design goes in a README.txt file or a sheet in the Excel file containing the data. Keep the readme information close to the data file. Also, information about who is the owner of the data or who performed the experiment when and where and with what type of device or reagents is very useful to include.
In our exercise above such README information would have saved us a lot of time figuring out what is what in the Excel file don't you think?

An example of a readme file is depicted below. 
 
```{r, dpi = 30, echo=FALSE}
knitr::include_graphics(
  here::here(
    "images",
    "readme.png"))
```
 
A short example of a README.txt file. It does not need to be very long, but provides information on where the data (which project?) refers to, who the owners are, who to contact in case of questions and what are the contents of the data (variable description) 

## Variable encodings
 
 - Use explicit encoding: male/female instead of `0`/`1`
 - Encodings can always be altered programmatically
 - Be consistent <mark>(see next graph)</mark>
 - Write a code journal that explains encodings, including units and levels
 - Use factors if a variable has a set of discrete possible outcomes: `sex`, `species`, `marital_status` etc
 - Use an ordered factor if there is a hiearchy in the factor levels: `year`, `month`, `number_of_legs`

Here we use the `{palmerpenguins}` dataset as an example to show you how to deal with encoding variables.

<p style="font-size:14px">[palmerpenguins](https://github.com/allisonhorst/palmerpenguins)</p> 

```{r}
# install.packages("remotes")
# remotes::install_github("allisonhorst/palmerpenguins")
library(palmerpenguins)
data_penguins <- palmerpenguins::penguins_raw 
data_penguins
```

### How to add variable encodings
<mark> XXX </mark>




### Factor levels

R (unlike SPSS) does not mind if you use descriptive words instead of numbers as categorical variable values: 

```{r, echo=TRUE}
data_penguins %>%
  ggplot(aes(x = Sex, y = `Flipper Length (mm)`)) +
  geom_point(aes(colour = Species), position = "jitter", show.legend = FALSE)
unique(data_penguins$Sex) ## we also call this factor levels
```


## Data-formats - Non-Proprietary
When we store data for re-use, we need it to be in an interoperable form. This means that it can be read (also after a long time - let's say 30 years from now) into analysis software. This can be achieved by storing data in a so-called `non-proprietary` format. This means basically that the format source code is open and maintained by open source community or core development team.
Here are some examples: 

  >- `.netCDF` (Geo, proteomics, array-oriented scientific data)  
  >- `.xml` / `.mzXML` (Markup language, human and machine readable, metadata + data together)
  >- `.txt` / `.csv` (flat text file, usually tab, comma or semi colon (`;`) seperated) 
  >- `.json` (text format that is completely language independent) 
  >- `fastq` / `fasta` and their equivalents
  
These formats will remain readable, even if the format itself becomes obsolete

**When storing a curated dataset for sharing or archiving it is always better, and sometimes enforced by the repository, to choose a non-proprietary format**

## Data entry
Data entry preferably, must be performed in a project template. The template contains predefined information on the observations in the study. The blank information needs to be filled out by the person responsible for/or performing the data entry. Or in other words: think about how you will enter your data before gathering it, and if there are multiple people gathering the data, make sure that everyone uses the exact same way of entering the data (template).

Enter an “NA” for missing values, do not leave cells blank if there is a missing value. Use only “NA” and nothing else. If you want to add additional information on the “NA”, put that in the “remarks” column.

(By the way, you can visualise missing dtaa like this in R: )
```{r, dpi = 130}
naniar::vis_miss(data_penguins)
```

After entry (and validation) of the filled-out template, NEVER change a value in the data. If you want to make changes, increment the version number of the file and document the change in the README.txt file or sheet in an Excel file (see below)

## Data-log
In the root of the folder `\data` an MS Excel file is being kept to track all the files present in this folder. Provide names and additional information here. Meta information for the `\data_raw` folder is best kept in that folder in a `README.txt` file.

The folder `doc` contains documentation and can be basically everything concerning information about the project, not concerning the data. For example a PowerPoint presentation on the experimental design of a study, or a contract or something else. Data information goes in the “supporting’ folder that is in the same folder as where the data file it refers to is stored.

Use versioning of data files. Decide on a versioning system for yourself, an stick to it.

## README.txt or README sheet inside an Excel file
Meta information like type of variables, ranges, units, number of observations or subjects in a study, type of analysis goes in a README.txt file or a sheet in the Excel file containing the data. Keep the readme information close to the data file.
An example of a readme file is depicted below.

```{r, dpi = 50, echo=FALSE}
knitr::include_graphics(
  here::here(
    "images",
    "readme.png"))
```

A short example of a README.txt file. It does not need to be very long, but provides information on where the data (which project?) refers to, who the owners are, who to contact in case of questions and what are the contents of the data (variable description)

A tidy data template is avaialable [here](https://github.com/uashogeschoolutrecht/efro-docs/blob/main/data/CE.LIQ.120_tidydata.xlsx)

When using this template, please be aware the following pointers:

For Excel users:

 - Start your file in A1 of a clean new sheet.
 - Use row 1 for variable names, use the rest of the rows for observations
 - It is allowed to have existing sheets in the file. Label the new tidy formatted datasheet as "tidy"
 - Do not fuse cells
 - Provide one value per cell so e.g. put units in a separate column
 - Adhere to the naming conventions for variable names (see above)
 - Use cell validation for entered values. This is especially true if you have multiple categorical variables that need manual labeling in Excel. A typo is just around the corner.


## Annotations and meta data in multiple files
In order to reduce effort in generating a complete tidy table for your data it might be worthwhile to create a number of extra tables containing meta data.
Typically this is how it would work:

Assume we have wide data format originally created in Excel looking like this
```{r}
measured1 <- rbinom(100, size = 2, prob = 0.3)
measured2 <- rnorm(100, mean = 5.3, sd = 0.1)
measured3 <- rnbinom(100, size = 10, prob = 0.1)
concentration <- rep(1:10, 10)
data <- tibble::tibble(
  `concentration (mMol/l)` = concentration,
  `measured 1 (pg/ml)` = measured1,
  `measured 2 (ng/ml)` = measured2,
  `measured 3 (ng/ml)` = measured3
)
data
```

The `r names(data)` are the variable names as provided in Excel. As you can see they are not adherent in a few ways tot the aforementioned naming conventions. The `r names(data)[2:ncol(data)]` refer to three variables that were determined in some experiment. The units of measurements are (as is common in Excel files) mentioned between brackets in the column name.

For compatibility and interoperability reasons this data format can be improved to a more machine readable format. There are two solutions that could be provided here. One is more laborious to achieve manually (solution 2) than the other (solution 1). If there is no expertise for scripting reformatting and cleaning up data is available in the research group, I recommend solution 2.

### Solution 1 (requires programming skills)
Add meta data and variable information to a separate table. The strategy is to keep all experimentally relevant data that is related to the experimental design in one sheet and all additional data (metadata or also called 'data about the data'). In this case, the unit information that is included in the variable names can be considered metadata. So as an example, I will include that information in a separate table that I will call coldata (short for column data)

First we need to create a pivoted table where the first column represents the variable names of our `data` table. Than we need to add a row for each variable in our data. It is best if the variable names and the values in the meatdata table in column 1 excactly match (in term of spelling and typesetting). I will show how this looks for our data
```{r}
var_names <- names(data)
metadata <- tibble::tibble(
  varnames = var_names
)
metadata
```

We now have a `metadata` table with one column called `varnames`. However, we are not done. If we want to create a tidy format of our metadata table we need to separate the unit information from the variable names column. Let's extract the units into it's own column

```{r}
metadata %>%
  mutate(
    varnames = str_replace_all(
      varnames,
      pattern = " ",
      replacement = "")) %>%
  separate(
    varnames,
    into = c("varnames", "units"), sep = "\\(", remove = FALSE) %>%
  mutate(
    units = str_replace_all(
      units,
      pattern = "\\)",
      replacement = "")) -> metadata_clean
metadata_clean
```

We can now start adding additonal information such as remarks or methods to the the metadata column.

```{r}
methods <- c("dilution", "elisa", "lcms", "flow cytometry")
remarks <- c(
  "concentration of exposure compound",
  "compound x is related to elevated blood pressure"
  )
```


## Portfolio assignment `r paste0(les, " ")` {-}
<div class = "dagopdracht">

**Applying the Guerrilla analytics framework to your own project.**

(A) 
 1. Look at your RStudio project that you created for the DAUR-II final assignment
 1. Rearrange your project according the Guerilla principles explained above
 1. Add README files to the datasets 
 1. Use the `{fs}` package to share a screenshot of your folder tree in your portfolio, look [at:](https://www.tidyverse.org/blog/2018/01/fs-1.0.0/) for more info on how to use the `{fs}` package. 

(B)
Now clean up your work environment for this course (workflows) and the parallel course in DSFB2 (projecticum). Set up a folder structure that will accomodate future plans and collaboration on the projecticum. Provide readme-files or comments within the code where needed. 
</div>

## Resources

 - [F1000 'Data guidelines'](https://f1000research.com/for-authors/data-guidelines)
 - [Guerrilla Analytics](https://guerrilla-analytics.net/the-principles/)
 - [Guerrilla Analytics 'slides'](https://www.slideshare.net/guerrillaanalytics/2014-10-newcastle-university-02web)
 - [F1000 'Preparing your data for sharing'](https://f1000research.com/for-authors/data-guidelines#prepareyourdata)
