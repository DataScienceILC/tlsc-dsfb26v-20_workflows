# Lesson 1 - Part 1 - Reproducible Research Introduction {#represintro}

## Exercise and assignment directives
Please answer all the assignment and exercises in an RMarkdown document. Save it somewhere in your course project. You will need to hand this in later in Github.com.

This is part 1 of lesson 1. Lesson 1 consists of:

 - Part 1; Introducing Reproducible Research
 - Part 2; Managing your project files and data with 'Guerilla Analytics'
 - Part 3; RMarkdown Driven Development 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      error = FALSE,
                      message = FALSE)

image_dir <- here::here(
  "images"
)
chapter_name <- "represintro"
lesson_number <- c("1.1")

``` 

## Packages
```{r}
library(tidyverse)
library(nlme)
```

```{r, topic, include=FALSE}
## add topics below to write them automatically to the course contents table
topics <- c(
  "Reproducible research",
  "Open source",
  "Guerilla analytics",
  "software development",
  "Literate programming",
  "Programming vs GUI",
  "R Markdown",
  "R Markdwon Driven Development",
  "R package development",
  "Git/Github",
  "Version Control"
)

tibble(
  Lesson_Number = lesson_number,
  Lesson_Name = chapter_name,
  Topics = topics
  ) %>%
  write_csv(file = "course_contents.csv", 
            append = TRUE)

```

## Resources

https://www.displayr.com/what-is-reproducible-research/

https://book.fosteropenscience.eu/en/02OpenScienceBasics/04ReproducibleResearchAndDataAnalysis.html

## Lesson Contents

This lesson is about the 'movement' in science called 'Reproducible Research'. It explains the principles of Open Science and connected terms such as Open Access, Open Source and Reproducible Research, and Open Data.

## Introducing Reproducible Research

[This fragment is adapted from:](https://book.fosteropenscience.eu/en/02OpenScienceBasics/04ReproducibleResearchAndDataAnalysis.html)

Reproducibility means that research data and code are made available so that others are able to reach the same results as claimed in scientific outputs. Closely related is the concept of replicability, the act of repeating a scientific methodology to reach similar conclusions. These concepts are core elements of empirical research. Thus far, the focus in your education has been on the replicability principle. In order to increase the quality of your research output, we now turn our focus to reproducibility in this course.

Improving reproducibility leads to increased rigour and quality of scientific outputs, and thus to greater trust in science. There has been a growing need and willingness to expose research workflows from initiation of a project and data collection right through to the interpretation and reporting of results. These developments have come with their own sets of challenges, including designing integrated research workflows that can be adopted by collaborators while maintaining high standards of integrity.

The concept of reproducibility is directly applied to the scientific method, the cornerstone of Science, and particularly to the following five steps:

 - Formulating a hypothesis
 - Designing the study
 - Running the study and collecting the data
 - Analyzing the data
 - Reporting the study

Each of these steps should be clearly reported by providing clear and open documentation, and thus making the study transparent and reproducible. Many factors can contribute to the causes of non-reproducibility, but can also drive the implementation of specific measures to address these causes. The culture and environment in which research takes place is an important 'top-down' factor. From a 'bottom-up perspective, continuing education and training for researchers can raise awareness and disseminate good practice. This is the central reason why we adress this issue and provide you with the tools to do reproducible research in this course

While understanding the full range of factors that contribute to reproducibility is important, it can also be hard to break down these factors into steps that can immediately be adopted into an existing research program and immediately improve its reproducibility. One of the first steps to take is to assess the current state of affairs, and to track improvement as steps are taken to increase reproducibility even more. 

![Some of the common issues with research reproducibility are shown here.](https://book.fosteropenscience.eu/en/Images/image_2.png){ width=60%}

[Source: Symposium report, October 2015. Reproducibility and reliability of biomedical research: improving research  practice](https://acmedsci.ac.uk/viewFile/56314e40aac61.pdf)

## Open Science

Open Science is the practice of science in such a way that others can collaborate and contribute, where research data, lab notes and other research processes are freely available, under terms that enable reuse, redistribution and reproduction of the research and its underlying data and methods (FOSTER Open Science Definition). In a nutshell, Open Science is transparent and accessible knowledge that is shared and developed through collaborative networks (Vicente-Sáez & Martínez-Fuentes 2018).

Open Science is about increased rigour, accountability, and reproducibility for research. It is based on the principles of inclusion, fairness, equity, and sharing, and ultimately seeks to change the way research is done, who is involved and how it is valued. It aims to make research more open to participation, review/refutation, improvement and (re)use for the world to benefit.

## Why we need Reproducible (Open) Science?

 >- To assess validity of science and methods we need access to data, methods and conclusions
 >- To learn from choices other researchers made
 >- To learn from omissions, mistakes or errors
 >- To prevent publication bias (also negative results will be available in reproducible research)
 >- To be able to re-use and/or synthesize data (from many and diverse sources)
 >- To have access to it all!
 
<p style="font-size:14px">[Nature Collection on this topic](https://www.nature.com/collections/prbfkwmwvz)</p>
 
## The _GUI problem_

How would you 'describe' the steps of an analysis or creation of a graph when you use GUI* based software? 

_"You can only do this using code, so it is (basically) impossible in a GUI"_**

```{r, dpi = 80}
knitr::include_graphics(
  file.path(
    image_dir,
    "messy_steps.jpg"
  )
)
```

<p style="font-size:14px">*[Graphical User Interface (GUI)...is a form of user interface that allows users to interact with electronic devices through graphical icons and audio indicator such as primary notation, instead of text-based user interfaces, typed command labels or text navigation...](https://en.wikipedia.org/wiki/Graphical_user_interface)</p>

<p style="font-size:14px">**The file "./Rmd/steps_to_graph_from_excel_file.html" shows you how to do this using the programming language R. In webinar part 3, we will revisit this example.</p>

## To  replicate a scientific study we need at least:

 > - Scientific context, research questions and state of the art [<mark>P<mark/>]
 > - (Experimental) model or characteristics of population or matter studied [<mark>P</mark>]
 > - Data that was generated and corresponding meta data [<mark>D</mark>, _C_]
 > - **Exact** (experimental) design of the study [<mark>P</mark>, _D_, <mark>C</mark>]
 > - Exploratory data analysis of the data [_P_, <mark>C</mark>]
 > - **Exact** methods that were used to conduct any formal inference [_P_, <mark>C</mark>]
 > - Model diagnostics [_C_]
 > - Interpretations of the (statistical) model results/model fitting process [_P_, _C_]
 > - Conclusions and academic scoping of the results [<mark>P</mark>, _C_]
 > - **Access to all of the above** [<mark>OAcc, OSrc</mark>]

<p style="font-size:14px">$P = Publication$, $D = Data$, $C = Code$, $OAcc = Open\ Access$, $OSrc = Open\ Source$ </p> 

So we can conclude that 

$Reproducible\ (Open)\ Science =$ 
$Reproducible\ Research + Open\ Science$

## EXERCISE

Is (hydroxy)chloroquine really an option for treating COVID-19?
As you probably know, hydroxychloroquine was repeatedly touted as a promising cure for COVID-19 by US President Donald Trump in 2020. 

```{r, dpi = 70}
knitr::include_graphics(
  file.path(
    image_dir,
    "trump_chloroquine.png"
  )
)
```

<p style="font-size:14px">https://www.washingtonpost.com/politics/2020/04/07/trumps-promotion-hydroxychloroquine-is-almost-certainly-about-politics-not-profits/</p>

## But how are we really doing with (hydroxy)chloroquine as a treatment for COVID-19?

```{r}
knitr::include_graphics(
  file.path(
    image_dir,
    "lancet_covid.png"
  )
)

```

<p style="font-size:14px">https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)31180-6/fulltext</p>

**Answer the following questions**

a) What was the reason for retracting this [paper](https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)31180-6/fulltext)?

```{r, include=FALSE, eval=FALSE}
# <p style="font-size:18px">_"Our independent peer reviewers informed us that Surgisphere would not transfer the full dataset, client contracts, and the full ISO audit report to their servers for analysis as such transfer would violate client agreements and confidentiality requirements"_</p>
# 
#  - Company Surgisphere ('data owner') did not share raw data
#  - At time of publication (raw) data and analysis (code) was not included in the manuscript
#  - The authors initiated the retract
# 
# <p style="font-size:14px">https://www.sciencemag.org/news/2020/06/two-elite-medical-journals-retract-coronavirus-papers-over-data-integrity-questions</p>
```

 B) Why is the retraction in itself a problem?

```{r, include = FALSE, eval=FALSE}
 # >- Scientific conclusions get picked up by the media, retracting statements is difficult
 # >- The credibility of the Journal, the researchers and the affiliated institutions are at stake (people got sacked over this!)
 # >- Clinical studies to hydroxy(choloroquine) were halted because of this paper 
 # >- The credibility of the company Surgisphere is at stake (they should have prevented this...)
 # >- The credibility of Science as a whole is at stake ('in the eye of the beholder')
```

 C)  In the sense that we defined above, does The Lancet adhere to Reproducible (Open) Science? Why not?

```{r}
# Would the Lancet have adopted the Reproducible (Open) Science framework:
# 
#  >- There would have been no publication, so no retraction necessary
#  >- The manuscript of this paper would not even have made it through the first check round
#  >- All data, code, methods and conclusions would have been submitted 
#  >- This would have enabled a complete and thorough peer-review process that includes replication of (part of) the data analysis of the study
#  >- Focus should be on the data and methods, not on the academic narratives and results ...
#  >- In physics and bioinformatics this is already common practice 
```


## Data, methods and logic {.build}

So in order to perform Reproducible Open Science we need to make a full connection between the data, the analysis and the narratives (the conclusions, the 'story', etc.)
In *[Brown, Kaiser & Allison, PNAS, 2018](https://doi.org/10.1073/pnas.1708279115)*
We read:

"...in science, three things matter:

 >1. the data, 
 >1. the methods used to collect the data [...], and 
 >1. the logic connecting the data and methods to conclusions,

everything else is a distraction."

So what happens if we do not adhere to this principle?

## `Gollums` lurking about {.build}

```{r, dpi = 80}
knitr::include_graphics(
  file.path(
    image_dir,
    "gollum_climbing.jpg"
  )
)
```

## Why using Excel for Biology is a bad idea
When you use Excel for data collection and especially for genetics and genomics studies, you risk losing and messing up your data. This is due to the 'autoformatting' in Excel. This is nicely adressed in this [paper:](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-1044-7). The authors show, through an automated workflow, that many Excel files used for genomics studies and that were supplied as supplementary materials to a publication contains 'autoformatted' _changed_ gene names. 

In itself this 'autoformatting' is problematic enough, but when you have data containing time series or dates, things start to become really problematic when you use Excel.

Also, if only one record in a whole Excel file is changed to the wrong data-type, R will parse this column most likely as a character. This stresses the need for thourougly checking your data types, after importing Excel files into R.

We will practice a bit to see this happening live in an Exercise 

## EXERCISE

 A) In R, try downloading the following [file:](http://genesdev.cshlp.org/content/suppl/2009/06/11/gad.1806309.DC1/FancySuppTable2.xls)
 B) What problems do you encounter, when trying to import this Excel file using `{readxl}`?
 C) Perform a manual download of this file by going [here:](http://genesdev.cshlp.org/content/suppl/2009/06/11/gad.1806309.DC1). You need the file under the link `Supp Table 2.xls`
 D) Why is manually downloading this file a problem?
 E) Import the manually downloaded Excel file into R 
 F) Open the file and compare the contents of the file to the Supplementary table shown [here](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-1044-7#Sec1). What do you observe? Why do you think this is happening?
 G) Look at this article in the Microsoft help: https://support.microsoft.com/en-us/office/stop-automatically-changing-numbers-to-dates-452bd2db-cc96-47d1-81e4-72cec11c4ed8
 H) Change the column `Example Gene Name Conversion` in the file 
[here:](http://genesdev.cshlp.org/content/suppl/2009/06/11/gad.1806309.DC1) to text. 
 I) Change the column named `GENE SYMBOL` in [this file](http://genesdev.cshlp.org/content/suppl/2009/11/03/23.21.2484.DC2/Bilodeau_Supp_Table_11.xls) to text.
 J) Now try to find the wrongly converted gene called `40059` in this datafile
 H) On the basis of your observations in this exercise: what are your conclusions about using Excel. How could you make the use of Excel _safer_ for data collection and analysis?

```{r, parse_failure, include=FALSE, eval=FALSE}
url <- c("http://genesdev.cshlp.org/content/suppl/2009/06/11/gad.1806309.DC1/FancySuppTable2.xls")

download.file(
  url = url,
  destfile = "FancySuppTable2.xls"
  )

fancy_supp <- read_csv(
  here::here(
    "FancySuppTable2.xls"
  )
)


url2 <- c("http://genesdev.cshlp.org/content/suppl/2009/11/03/23.21.2484.DC2/Bilodeau_Supp_Table_11.xls")

download.file(
  url = url2,
  destfile = "bilodeau.xls"
  )

bilodeau_supp <- readxl::read_xls(
  here::here(
    "bilodeau.xls"
  )
)

# Error: 
#...\bilodeau.xls
#   libxls error: Unable to open file
```


--------------- EXERCISE END ------------------------------

## EXERCISE

The data for this exercise was derived from an experiment in which adult _C.elegans_ nematodes were exposed to varying concentrations of different compounds. The variables `RawData` (the outcome - number of offspring counted as an integer value, after incubation time), `compName` (the generic name of the compound/chemical), the `compConcentration` (the concentration of the compound), and the `expType` are the most important variables in this dataset.
A typical analysis with this data would be to run a dose-response analysis using a log-logistic model with estimates for the maximal, the minimal, the IC50 concentration and the slope at IC50. We will not go into the details but a good package to run such computations and create graphs in R is the `{drc}` package. [See:](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0146021) [and:](https://cran.r-project.org/web/packages/drc/index.html). In the exercise below we will create some visualizations using `{ggplot2}`.

A) Review the following Excel file in the `./data/CE.LIQ.FLOW.062_Tidydata.xlsx`, by opening the file in Excel. See if you can spot anything perculiar about this file. Do not edit the file in any way. Just close it when you are done
 
B) Open the file in R, using the `{readxl}` package.
 
```{r, include=FALSE}
celiqflow_062 <- readxl::read_xlsx(
  here::here(
    "data",
    "CE.LIQ.FLOW.062_Tidydata.xlsx"
  )
)
```

C) Inspect the data types of columns `RawData`, `compName` and `compConcentration`. What types would you expect from the experimental description above. Have the datatypes been correctly assigned during the importing of the data into R?

```{r, include=FALSE}
names(celiqflow_062)
map(celiqflow_062, typeof)
```

D) Create a graph displaying a scatterplot for the `CE.LIQ.FLOW.062_Tidydata.xlsx` data, for the different compounds and the varying concentrations. Put the `compConcentration` on the x-axis, the `DataRaw` counts on the y-axis and assign a colour to each level in `compName`. Assign a different symbol (`shape = `) to each level in the `expType` variable. Try fixing the labels of the x-axis so that we can read them.

**TIPS** 
I always have difficulty remembering how to rotate axis labels in R. That is why I wrote an R package that has a convenient function. The package can be installed by running:
```
devtools::install_github("uashogeschoolutrecht/toolboxr")
## load the package
library(toolboxr)
```
The function you need from `{toolboxr}` is `rotate_axis_labels()`. Type
`?rotate_axis_labels()` in the R Console to see how to use this function. 

If you have any suggestions for a good convenience function to add to this package, create an issue on github.com for a feature request or clone/fork the repository, add the fucntion you would like to add and create a pull request. You will learn how to take this steps in lesson 2 and 3.

```{r}
celiqflow_062 %>%
  ggplot(aes(x = compConcentration, 
             y = RawData)) +
  geom_point(aes(colour = compName,
                 shape = expType)) +
  toolboxr::rotate_axis_labels("x", 45)
  
```

E) When creating the plot under C), what happened with the ordering of the x-axis labels. Explain why this happens. Look at the data-type of the `compConcentration` column in the data again to find a clue.

F) Correct the data-type of `compConcentration` to `numeric` and than look at the graph again. Use a log10 transformation on the x-axis to get a clear graph. Also, add a bit of `jitter` to the points in the graph so that points are not overlapping.  
```{r}
## TODO: add jitter, does not work
celiqflow_062 %>%
  mutate(compConcentration = as.numeric(compConcentration)) %>%
  ggplot(aes(x = log10(compConcentration + 0.001), 
             y = RawData)) +
  geom_point(aes(colour = compName,
                 shape = expType)) +
  toolboxr::rotate_axis_labels("x", 45)
```

Fill in:
G) The positive control for this experiments is .....
H) The negative control for this experiment is .....
I) Think about how you would analyse this experiment to learn whether there is indeed an effect of different concentrations on offspring count and whether the different compounds have a different curve (IC50). Write down you analysis as a step-wise plan 
J) Normalize the data for the `controlNegative` in such a way that the mean value for `controlNegative` is exactly equal to 1 and that all other values are expressed as a fraction thereof.

--------------- EXERCISE END ------------------------------

## When things go terribly wrong

In *[Brown, Kaiser & Allison, PNAS, 2018](https://doi.org/10.1073/pnas.1708279115)* we also read:

 "In one case, a group accidentally used reverse-coded variables, making their conclusions the opposite of what the data supported."

 "In another case, authors received an incomplete dataset because entire categories of data were missed; when corrected, the qualitative conclusions did not change, but the quantitative conclusions changed by a factor of >7"

These are common mistakes and if no checks or tests are build in than the conclusions from a study could be completely wrong. This does not only lead to bad science, it can be dangerous.

How would you mitigate such a risk?
We can start by doing everything we do in code via scripting. The prefect way to do this is, is by using literate programming such as is done when using RMarkdown. In this course we call this `RMarkdown driven development`

## Programming is essential for Reproducible (Open) Science {.build}

 >- Only programming an analysis (or creation of a graph) records every step
 >- The script(s) function as a (data) analysis journal 
 >- Code is the logic that connects the data and methods to conclusions 
 >- Learning to use a programming language takes time but pays of at the long run (for all of science)

**(Literate) programming is a way to connect narratives to data, methods and results**

```{r}
knitr::include_graphics(file.path(image_dir,"rmd_printscr.png"))
```

## A short example of Reproducible (Open) Science

Assume we have the following question:
"Which of 4 types of chairs takes the least effort to arise from when seated in?"

We have the following setup:

 - 4 different types of chairs
 - 9 different subjects (probably somewhat aged)
 - Each subject is required to provide a score (from 6 to 20, 6 being very lightly strenuous, 20 being extremely strenuous) when arising from each of the 4 chairs. There is some 'wash-out' time in between the trials. The chair order is randomised.

To analyze this experiment statistically, the model would need to include: the rating score as the **measured (or dependent) variable**, the type of chair as the **experimental factor** and the subject as the **blocking factor**

## Mixed effects models

A typical analysis method for this type of randomized block design is a so-called 'multi-level' or also called 'mixed-effects' or 'hierarchical' models. An analysis method much used in clinical or biological scientific practice. 
 
You could also use one-way ANOVA but I will illustrate why this is not a good idea 

## What do we minimally need, to replicate the science of this experiment? {.build}

I will show:

 >- the data 
 >- an exploratory graph 
 >- a statistical model 
 >- the statistical model results
 >- a model diagnostic
 >- some conclusions 
 
In the next few slides, I will hopefully convince you of the power of (literate) programming to communicate such an analysis. 

<p style="font-size:14px">[Example reproduced from: Pinheiro and Bates, 2000, _Mixed-Effects Models in S and S-PLUS_, Springer, New York.](https://cran.r-project.org/web/packages/nlme/index.html)</p>
 
## The data of the experiment

<p style="font-size:14px">[Wretenberg, Arborelius & Lindberg, 1993](https://doi.org/10.1080/00140139308967910)</p>


```{r, echo=TRUE}
library(nlme)
ergoStool %>% as_tibble()
```

## An exploratory graph
```{r}
set.seed(123)
plot_ergo <- ergoStool %>%
  ggplot(aes(x = reorder(Type, effort), y = effort)) + 
  geom_boxplot(colour = "darkgreen", outlier.shape = NA) + 
  geom_jitter(aes(colour = reorder(Subject, -effort)), 
              width = 0.2, size = 3) +
  scale_colour_manual(values = c("red","blue", "green", "darkblue", "darkgreen", "purple", "grey", "black", "darkgrey")) +
  ylab("Effort (Borg scale score)") +
  xlab("Chair type") + 
  guides(colour=guide_legend(title="Subject id")) +
  theme_bw()
plot_ergo
```

## Mind the variability per subject, what do you see?

 - Can you say something about within-subject variability (note 'Minster Blue')?
 - Can you say something about between-subject variability (note 'Mister Green', vs 'Mister Black')?
 - Which chair type takes, on average the biggest effort to arise from?
 
```{r, fig.width=5, fig.height=3}
plot_ergo
```

## The statistical questions

 1. Which chair type takes, on average the biggest effort to arise from? (ANOVA / MEM, fixed effects)
 - Do individual (within subject) differences play a role in appointing a average score to a chair type? (MEM, random effects)
 - Does variability between subjects play a role in determining the 'best' chair type (ANOVA / MEM, confidence intervals)

## The statistical model 
Statistical models (in R) can be specified by a `model formula`. The left side of the formula is the dependent variable, the right side are the 'predictors'. Here we include a `fixed` and a `random` term to the model (as is common for mixed-effects models)

```{r, echo=TRUE, eval=FALSE}
library(nlme)
```
```{r, echo=TRUE}
ergo_model <- lme(
  data = ergoStool, # the data to be used for the model
  fixed = effort ~ Type, # the dependent and fixed effects variables
  random = ~1 | Subject # random intercepts for Subject variable
)
```

<p style="font-size:18px">The `lme()` function is part of the [`{nlme}`](https://cran.r-project.org/web/packages/nlme/index.html) package for mixed effects modelling in R</p>

<p style="font-size:18px">Example reproduced from: [Pinheiro and Bates, 2000, _Mixed-Effects Models in S and S-PLUS_, Springer, New York.](https://cran.r-project.org/web/packages/nlme/index.html)</p>

## The statistical results
```{r}
result <- ergo_model %>% summary() 
result$tTable %>% as.data.frame() %>% knitr::kable()
```

## Model diagnostics {.build}

 >- Diagnostics of a fitted model is the most important step in a statistical analysis
 >- In most scientific papers the details are lacking 
 >- Did the authors omit to perform this step? Or did they not report it?
 >- If you do not want to include it in your paper, put it in an appendix!
 
A residual plot shows the 'residual' error ('unexplained variance') after fitting the model. Under the Normality assumption standardized residuals should:
 
 >1. Be normally distributed around 0
 >1. Display no obvious 'patters'
 >1. Should display overall equal 'spread' above and below 0 ('assumption of equal variance')
 
## Residual plot
```{r, echo=TRUE}
plot(ergo_model) ## type = 'pearson' (standardized residuals)
```

## The conclusions in a plot
```{r}
# install.packages("ggsignif")
library(ggsignif)
p_values <- result$tTable %>% as.data.frame()
annotation_df <- data.frame(Type=c("T1", "T2"), 
                            start=c("T1", "T1"), 
                            end=c("T2", "T3"),
                            y=c(16, 14),
                            label=
                              paste("p-value:",
                              c(
                              formatC(
                                p_values$`p-value`[2], digits = 3),
                              formatC(
                                p_values$`p-value`[3], digits = 3)
                              )
                            )
                          )
                            
set.seed(123)
ergoStool %>%
  ggplot(aes(x = reorder(Type, effort), 
             y = effort)) + 
  geom_boxplot(colour = "darkgreen", 
               outlier.shape = NA) + 
  geom_jitter(aes(
    colour = reorder(Subject, -effort)), 
    width = 0.2, 
    size = 3) +
  scale_colour_manual(
    values = c(
      "red", "blue","green", 
      "darkblue", "darkgreen", 
      "purple", "grey", "black", 
      "darkgrey")) +
  ylab("Effort (Borg scale score)") +
  xlab("Chair type") + 
  guides(colour=guide_legend(title="Subject id")) +
  ylim(c(6,20)) +
  geom_signif(
    data=annotation_df,
    aes(xmin=start, 
    xmax=end, 
    annotations=label, 
    y_position=y),
    textsize = 5, vjust = -0.2,
    manual=TRUE) +
  theme_bw() -> plot_ergo
plot_ergo
```

## And the most important part...

odz: _Practice what you preach_

If you want to reproduce, add-on, falsify or apply your own ideas to this example, you can find the code (and data) in [Github.com](https://github.com/uashogeschoolutrecht/work_flows)

**In webinar 3, I will show you how to actually run, use and organize code like this!**

```{r, dpi = 60}
knitr::include_graphics(
  file.path(image_dir,
  "git_collaboration.png")
)
```

## Thank you for your attention!

```{r, dpi = 150}
knitr::include_graphics(
  file.path(
    image_dir,
    "Pepper.png")
)
```

**UPCOMING WEBINARS:**

 - Part 2; Managing your project files and data with 'Guerilla Analytics' 
 (~June 23rd, 2020)
 - Part 3; Reproducible (Open) Science @HU - Tools (~July 6th, 2020)
 
 [Peer Support Group Data Science](tln.hu.nl)
 [support voor onderzoek](https://bibliotheek.hu.nl/onderzoekers/)

## Example; The Open Science Framework [OSF](https://osf.io/)
```{r}
knitr::include_graphics(
  here::here(
    "images",
    "cos-shield.png")
)
```

$Reproducible\ Science = P + D + C + OAcc + OSrc$ 

**OSF has it all**

<p style="font-size:14px">$P = Publication$, $D = Data$, $C = Code$, $OAcc = Open\ Access$, $OSrc = Open\ Source$ </p> 

## OSF - Reproducible Project: Psychology

 >- 100 publications in Psychology journals
 >- Results from half of these publications could be reproduced
 >- Full access to P, D and C in [OSF](https://osf.io/ezcuj/)
 >- The publication is not published in an OAcc journal but:
 >- [The submitted manuscript is available in OSF](http://pps.sagepub.com/content/7/6/657.abstract)
 >- [The R code used is available in OSF](https://osf.io/fkmwg/)  
 
 $RP:Psychology = P + D + C + OSrc\ (+ OAcc)$

<p style="font-size:14px">$P = Publication$, $D = Data$, $C = Code$, $OAcc = Open\ Access$, $OSrc = Open\ Source$ </p> 

## <mark>ASSIGNMENT; "Assing a reproducibility score for scientific literature"</mark> {-}

resource: https://www.researchgate.net/publication/340244621_Reproducibility_and_reporting_practices_in_COVID-19_preprint_manuscripts/fulltext/5e81f9fd92851caef4ae37ba/Reproducibility-and-reporting-practices-in-COVID-19-preprint-manuscripts.pdf

This exercise is about identifying reproducibility issues in a scientific publication. We use the criteria for reproduciblity that are publically available [via:](https://www.researchgate.net/deref/https%3A%2F%2Fwww.ripeta.com%2Fuploads%2F7%2F4%2F8%2F7%2F7487334%2Fripeta_approach_and_criteria_definitions.pdf)

| Transparency Criteria| Definition       | Response Type|
|---------|-------------------------------|----------|
|Study Purpose |A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective.| Binary| 
|Data Availability Statement | A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section.| Binary|
|Data Location | Where the article’s data can be accessed, either raw or processed.| Found Value|
|Study Location| Author has stated in the methods section where the study took place or the data’s country/region of origin.| Binary; Found Value|
|Author Review| The professionalism of the contact information that the author has provided in the manuscript.|Found Value|
|Ethics Statement | A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data.|Binary|
|Funding Statement| A statement within the manuscript indicating whether or not the authors received funding for their research.|Binary|
Code Availability | Authors have shared access to the most updated code that they used in their study, including code used for analysis. |Binary|

**Table clarification** The `Transparency Criteria` are criteria you need to score the article of your choice for. Read them carefully and discuss with another course participant if you do not understand them. 
Each Tranparance criterion comes with a `Definition` that explains the criterion in more details. These descriptions are particularly helpful to understand what the criterium entails and what to look for in the article. The `Response Type` is the actual score  

In this assisgment you need to find a scientific article yourself, using PubMed or another database or repository. Use only Open Access articles. Having an article in hand, go over the table above and score the article according the criteria. **<mark>Be sure to select a primary article that presents a study using data from a experimental work </mark>**. This can be laboratory experiments or _in silico_ experiments. Reviews and meta analysis are not suitable for this assignment     

To guide your search you can choose between these topics

 - "Coronavirus / COVID-19"
 - "The effects of compound on an organism / Toxicology"
 - "The effectiveness of a drug or treatment in an animal study"
 - "The effects of a compound investigated in a cell or organoid system"
 
**TIPS**

 - If you do not know where to start your literature search start here: https://www.biorxiv.org/
 - This assignment is not about the topic you select, so try to do that quickly
 - You may want to cheat and select an article that scores TRUE on the `Data Availability Statement`, beacause that enables you to use the this article again in one of the next assignments.
 

To complete the assignment, execute activity A to G  
 
 - A) Initiate an empty RMarkdown file in your RStudio enviroment and provide author and title (after the title of this exercise)
 - B) Search for a primary Open Access article on one of the above listed topics, using Pubmed Central
 - C) Read the article diagonally to check if is indeed a primary article describing emperical scientific findings. 
 - D) Include the reference to this article in your Rmd file
 - E) Score the article on the basis of the above 'Repita' criteria
 - F) Write an Rmarkdown report on your findings, including the table above and some information about the article such as general aim, short methods and results. If data is available, try including some  
 - G) Store the source Rmd and knitted HTML in a folder called 'Rmd' in your course RStudio project. You will need it again later in the course
 
## <mark>Assignment</mark>

To complete this assignment you will have to follow steps A-

 - A) Using the [OSF website](https://osf.io/), select a project that addresses an aspect of the SARS-Cov-2 virus. 
 - B) Select a project that has a dataset and R-code shared in the project environment. 
 - C) Have a look at the code. Describe in your own words what the code intents to achieve.
 - D) In terms of readibility of the code, how would you grade (1(very bad)-5(very good)) the code available.
 - E) Download the code and the data to a new RStudio project
 - F) Run the script or code that is available to reproduce at least 1 figure
 - G) When you encounter errors or flaws in the script, try fixing them and record your changes.
 - H) Taken together on a scale from 1 (very hard) to 5 (very easy), how much effort did it take you to reproduce the visualization from the project, report or article
 - I) Generate an RMarkdown script that contains the details on the project you selected, the code you used to create the visualization and your score for reproducibility.
 
 
 
