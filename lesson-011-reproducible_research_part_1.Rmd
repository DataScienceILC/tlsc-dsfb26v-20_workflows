# Lesson 1.1 - Reproducible Research - Introduction {#represintro}

This is part 1 of lesson 1. Lesson 1 consists of:

 - Part 1; Introducing Reproducible Research
 - Part 2; Managing your project files and data with 'Guerilla Analytics'
 - Part 3; RMarkdown Driven Development 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      error = FALSE,
                      message = FALSE)

image_dir <- here::here(
  "images"
)
chapter_name <- "represintro"
lesson_number <- c("1.1")

``` 

## Packages
```{r}
library(tidyverse)
library(nlme)
```

```{r, topic, include=FALSE}
## add topics below to write them automatically to the course contents table
topics <- c(
  "Reproducible research",
  "Open source",
  "Guerilla analytics",
  "software development",
  "Literate programming",
  "Programming vs GUI",
  "R Markdown",
  "R Markdwon Driven Development",
  "R package development",
  "Git/Github",
  "Version Control"
)

tibble(
  Lesson_Number = lesson_number,
  Lesson_Name = chapter_name,
  Topics = topics
  ) %>%
  write_csv(file = "course_contents.csv", 
            append = TRUE)

```

## Resources

https://www.displayr.com/what-is-reproducible-research/

https://book.fosteropenscience.eu/en/02OpenScienceBasics/04ReproducibleResearchAndDataAnalysis.html

## Lesson Contents

This lesson is about the 'movement' in science called 'Reproducible Research'.



## Introducing Reproducible Research

[This fragment is adapted from:](https://book.fosteropenscience.eu/en/02OpenScienceBasics/04ReproducibleResearchAndDataAnalysis.html)

Reproducibility means that research data and code are made available so that others are able to reach the same results as claimed in scientific outputs. Closely related is the concept of replicability, the act of repeating a scientific methodology to reach similar conclusions. These concepts are core elements of empirical research. Thus far, the focus in your education has been on the replicability principle. In order to increase the quality of your research output, we now turn our focus to reproducibility in this course.

Improving reproducibility leads to increased rigour and quality of scientific outputs, and thus to greater trust in science. There has been a growing need and willingness to expose research workflows from initiation of a project and data collection right through to the interpretation and reporting of results. These developments have come with their own sets of challenges, including designing integrated research workflows that can be adopted by collaborators while maintaining high standards of integrity.

The concept of reproducibility is directly applied to the scientific method, the cornerstone of Science, and particularly to the following five steps:

 - Formulating a hypothesis
 - Designing the study
 - Running the study and collecting the data
 - Analyzing the data
 - Reporting the study

Each of these steps should be clearly reported by providing clear and open documentation, and thus making the study transparent and reproducible. Many factors can contribute to the causes of non-reproducibility, but can also drive the implementation of specific measures to address these causes. The culture and environment in which research takes place is an important 'top-down' factor. From a 'bottom-up perspective, continuing education and training for researchers can raise awareness and disseminate good practice. This is the central reason why we adress this issue and provide you with the tools to do reproducible research in this course

While understanding the full range of factors that contribute to reproducibility is important, it can also be hard to break down these factors into steps that can immediately be adopted into an existing research program and immediately improve its reproducibility. One of the first steps to take is to assess the current state of affairs, and to track improvement as steps are taken to increase reproducibility even more. 

![Some of the common issues with research reproducibility are shown here.](https://book.fosteropenscience.eu/en/Images/image_2.png){ width=60%}

[Source: Symposium report, October 2015. Reproducibility and reliability of biomedical research: improving research  practice](https://acmedsci.ac.uk/viewFile/56314e40aac61.pdf)

## Open Science

Open Science is the practice of science in such a way that others can collaborate and contribute, where research data, lab notes and other research processes are freely available, under terms that enable reuse, redistribution and reproduction of the research and its underlying data and methods (FOSTER Open Science Definition). In a nutshell, Open Science is transparent and accessible knowledge that is shared and developed through collaborative networks (Vicente-Sáez & Martínez-Fuentes 2018).

Open Science is about increased rigour, accountability, and reproducibility for research. It is based on the principles of inclusion, fairness, equity, and sharing, and ultimately seeks to change the way research is done, who is involved and how it is valued. It aims to make research more open to participation, review/refutation, improvement and (re)use for the world to benefit.





$Reproducible\ (Open)\ Science =$ 
$Reproducible\ Research + Open\ Science$

## Is (hydroxy)chloroquine really an option for treating COVID-19?
As you probably know, hydroxychloroquine was repeatedly touted as a promising cure for COVID-19 by US President Donald Trump 

```{r, dpi = 70}
knitr::include_graphics(
  file.path(
    image_dir,
    "trump_chloroquine.png"
  )
)
```

<p style="font-size:14px">https://www.washingtonpost.com/politics/2020/04/07/trumps-promotion-hydroxychloroquine-is-almost-certainly-about-politics-not-profits/</p>

## But how are we really doing with (hydroxy)chloroquine as a treatment for COVID-19?

```{r}
knitr::include_graphics(
  file.path(
    image_dir,
    "lancet_covid.png"
  )
)

```

<p style="font-size:14px">https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)31180-6/fulltext</p>

## What was the reason for retracting this paper?

<p style="font-size:18px">_"Our independent peer reviewers informed us that Surgisphere would not transfer the full dataset, client contracts, and the full ISO audit report to their servers for analysis as such transfer would violate client agreements and confidentiality requirements"_</p>

 - Company Surgisphere ('data owner') did not share raw data
 - At time of publication (raw) data and analysis (code) was not included in the manuscript
 - The authors initiated the retract

<p style="font-size:14px">https://www.sciencemag.org/news/2020/06/two-elite-medical-journals-retract-coronavirus-papers-over-data-integrity-questions</p>

## Why is this a problem?

 >- Scientific conclusions get picked up by the media, retracting statements is difficult
 >- The credibility of the Journal, the researchers and the affiliated institutions are at stake (people got sacked over this!)
 >- Clinical studies to hydroxy(choloroquine) were halted because of this paper 
 >- The credibility of the company Surgisphere is at stake (they should have prevented this...)
 >- The credibility of Science as a whole is at stake ('in the eye of the beholder')

## The Lancet does not adhere to Reproducible (Open) Science {.build}

Would the Lancet have adopted the Reproducible (Open) Science framework:

 >- There would have been no publication, so no retraction necessary
 >- The manuscript of this paper would not even have made it through the first check round
 >- All data, code, methods and conclusions would have been submitted 
 >- This would have enabled a complete and thorough peer-review process that includes replication of (part of) the data analysis of the study
 >- Focus should be on the data and methods, not on the academic narratives and results ...
 >- In physics and bioinformatics this is already common practice 

## Data, methods and logic {.build}

*[Brown, Kaiser & Allison, PNAS, 2018](https://doi.org/10.1073/pnas.1708279115)*

"...in science, three things matter:

 >1. the data, 
 >1. the methods used to collect the data [...], and 
 >1. the logic connecting the data and methods to conclusions,

everything else is a distraction."

## `Gollums` lurking about {.build}

```{r, dpi = 80}
knitr::include_graphics(
  file.path(
    image_dir,
    "gollum_climbing.jpg"
  )
)
```

 "In one case, a group accidentally used reverse-coded variables, making their conclusions the opposite of what the data supported."

 "In another case, authors received an incomplete dataset because entire categories of data were missed; when corrected, the qualitative conclusions did not change, but the quantitative conclusions changed by a factor of >7"

 <p style="font-size:14px">[Brown, Kaiser & Allison, 2018; PNAS](https://doi.org/10.1073/pnas.1708279115)</p>

## Why we need Reproducible (Open) Science?

 >- To assess validity of science and methods we need access to data, methods and conclusions
 >- To learn from choices other researchers made
 >- To learn from omissions, mistakes or errors
 >- To prevent publication bias (also negative results will be available in reproducible research)
 >- To be able to re-use and/or synthesize data (from many and diverse sources)
 >- To have access to it all!
 
<p style="font-size:14px">[Nature Collection on this topic](https://www.nature.com/collections/prbfkwmwvz)</p>
 
## The _GUI problem_

How would you 'describe' the steps of an analysis or creation of a graph when you use GUI* based software? 

_"You can only do this using code, so it is (basically) impossible in a GUI"_**

```{r, dpi = 80}
knitr::include_graphics(
  file.path(
    image_dir,
    "messy_steps.jpg"
  )
)
```

<p style="font-size:14px">*[Graphical User Interface (GUI)...is a form of user interface that allows users to interact with electronic devices through graphical icons and audio indicator such as primary notation, instead of text-based user interfaces, typed command labels or text navigation...](https://en.wikipedia.org/wiki/Graphical_user_interface)</p>

<p style="font-size:14px">**The file "./Rmd/steps_to_graph_from_excel_file.html" shows you how to do this using the programming language R. In webinar part 3, we will revisit this example.</p>

## Programming is essential for Reproducible (Open) Science {.build}

 >- Only programming an analysis (or creation of a graph) records every step
 >- The script(s) function as a (data) analysis journal 
 >- Code is the logic that connects the data and methods to conclusions 
 >- Learning to use a programming language takes time but pays of at the long run (for all of science)

**(Literate) programming is a way to connect narratives to data, methods and results**

```{r}
knitr::include_graphics(file.path(image_dir,"rmd_printscr.png"))
```

## To  replicate a scientific study we need at least:

 > - Scientific context, research questions and state of the art [<mark>P<mark/>]
 > - (Experimental) model or characteristics of population or matter studied [<mark>P</mark>]
 > - Data that was generated and corresponding meta data [<mark>D</mark>, _C_]
 > - **Exact** (experimental) design of the study [<mark>P</mark>, _D_, <mark>C</mark>]
 > - Exploratory data analysis of the data [_P_, <mark>C</mark>]
 > - **Exact** methods that were used to conduct any formal inference [_P_, <mark>C</mark>]
 > - Model diagnostics [_C_]
 > - Interpretations of the (statistical) model results/model fitting process [_P_, _C_]
 > - Conclusions and academic scoping of the results [<mark>P</mark>, _C_]
 > - **Access to all of the above** [<mark>OAcc, OSrc</mark>]

<p style="font-size:14px">$P = Publication$, $D = Data$, $C = Code$, $OAcc = Open\ Access$, $OSrc = Open\ Source$ </p> 

## A short example of Reproducible (Open) Science

Assume we have the following question:
"Which of 4 types of chairs takes the least effort to arise from when seated in?"
We have the following setup:

 - 4 different types of chairs
 - 9 different subjects (probably somewhat aged)
 - Each subject is required to provide a score (from 6 to 20, 6 being very lightly strenuous, 20 being extremely strenuous) when arising from each of the 4 chairs. There is some 'wash-out' time in between the trials. The chair order is randomised.

To analyze this experiment statistically, the model would need to include: the rating score as the **measured (or dependent) variable**, the type of chair as the **experimental factor** and the subject as the **blocking factor**

## Mixed effects models

A typical analysis method for this type of randomized block design is a so-called 'multi-level' or also called 'mixed-effects' or 'hierarchical' models. An analysis method much used in clinical or biological scientific practice. 
 
You could also use one-way ANOVA but I will illustrate why this is not a good idea 

## What do we minimally need, to replicate the science of this experiment? {.build}

I will show:

 >- the data 
 >- an exploratory graph 
 >- a statistical model 
 >- the statistical model results
 >- a model diagnostic
 >- some conclusions 
 
In the next few slides, I will hopefully convince you of the power of (literate) programming to communicate such an analysis. 

<p style="font-size:14px">[Example reproduced from: Pinheiro and Bates, 2000, _Mixed-Effects Models in S and S-PLUS_, Springer, New York.](https://cran.r-project.org/web/packages/nlme/index.html)</p>
 
## The data of the experiment

<p style="font-size:14px">[Wretenberg, Arborelius & Lindberg, 1993](https://doi.org/10.1080/00140139308967910)</p>


```{r, echo=TRUE}
library(nlme)
ergoStool %>% as_tibble()
```

## An exploratory graph
```{r}
set.seed(123)
plot_ergo <- ergoStool %>%
  ggplot(aes(x = reorder(Type, effort), y = effort)) + 
  geom_boxplot(colour = "darkgreen", outlier.shape = NA) + 
  geom_jitter(aes(colour = reorder(Subject, -effort)), 
              width = 0.2, size = 3) +
  scale_colour_manual(values = c("red","blue", "green", "darkblue", "darkgreen", "purple", "grey", "black", "darkgrey")) +
  ylab("Effort (Borg scale score)") +
  xlab("Chair type") + 
  guides(colour=guide_legend(title="Subject id")) +
  theme_bw()
plot_ergo
```

## Mind the variability per subject, what do you see?

 - Can you say something about within-subject variability (note 'Minster Blue')?
 - Can you say something about between-subject variability (note 'Mister Green', vs 'Mister Black')?
 - Which chair type takes, on average the biggest effort to arise from?
 
```{r, fig.width=5, fig.height=3}
plot_ergo
```

## The statistical questions

 1. Which chair type takes, on average the biggest effort to arise from? (ANOVA / MEM, fixed effects)
 - Do individual (within subject) differences play a role in appointing a average score to a chair type? (MEM, random effects)
 - Does variability between subjects play a role in determining the 'best' chair type (ANOVA / MEM, confidence intervals)

## The statistical model 
Statistical models (in R) can be specified by a `model formula`. The left side of the formula is the dependent variable, the right side are the 'predictors'. Here we include a `fixed` and a `random` term to the model (as is common for mixed-effects models)

```{r, echo=TRUE, eval=FALSE}
library(nlme)
```
```{r, echo=TRUE}
ergo_model <- lme(
  data = ergoStool, # the data to be used for the model
  fixed = effort ~ Type, # the dependent and fixed effects variables
  random = ~1 | Subject # random intercepts for Subject variable
)
```

<p style="font-size:18px">The `lme()` function is part of the [`{nlme}`](https://cran.r-project.org/web/packages/nlme/index.html) package for mixed effects modelling in R</p>

<p style="font-size:18px">Example reproduced from: [Pinheiro and Bates, 2000, _Mixed-Effects Models in S and S-PLUS_, Springer, New York.](https://cran.r-project.org/web/packages/nlme/index.html)</p>

## The statistical results
```{r}
result <- ergo_model %>% summary() 
result$tTable %>% as.data.frame() %>% knitr::kable()
```

## Model diagnostics {.build}

 >- Diagnostics of a fitted model is the most important step in a statistical analysis
 >- In most scientific papers the details are lacking 
 >- Did the authors omit to perform this step? Or did they not report it?
 >- If you do not want to include it in your paper, put it in an appendix!
 
A residual plot shows the 'residual' error ('unexplained variance') after fitting the model. Under the Normality assumption standardized residuals should:
 
 >1. Be normally distributed around 0
 >1. Display no obvious 'patters'
 >1. Should display overall equal 'spread' above and below 0 ('assumption of equal variance')
 
## Residual plot
```{r, echo=TRUE}
plot(ergo_model) ## type = 'pearson' (standardized residuals)
```

## The conclusions in a plot
```{r}
# install.packages("ggsignif")
library(ggsignif)
p_values <- result$tTable %>% as.data.frame()
annotation_df <- data.frame(Type=c("T1", "T2"), 
                            start=c("T1", "T1"), 
                            end=c("T2", "T3"),
                            y=c(16, 14),
                            label=
                              paste("p-value:",
                              c(
                              formatC(
                                p_values$`p-value`[2], digits = 3),
                              formatC(
                                p_values$`p-value`[3], digits = 3)
                              )
                            )
                          )
                            
set.seed(123)
ergoStool %>%
  ggplot(aes(x = reorder(Type, effort), 
             y = effort)) + 
  geom_boxplot(colour = "darkgreen", 
               outlier.shape = NA) + 
  geom_jitter(aes(
    colour = reorder(Subject, -effort)), 
    width = 0.2, 
    size = 3) +
  scale_colour_manual(
    values = c(
      "red", "blue","green", 
      "darkblue", "darkgreen", 
      "purple", "grey", "black", 
      "darkgrey")) +
  ylab("Effort (Borg scale score)") +
  xlab("Chair type") + 
  guides(colour=guide_legend(title="Subject id")) +
  ylim(c(6,20)) +
  geom_signif(
    data=annotation_df,
    aes(xmin=start, 
    xmax=end, 
    annotations=label, 
    y_position=y),
    textsize = 5, vjust = -0.2,
    manual=TRUE) +
  theme_bw() -> plot_ergo
plot_ergo
```

## And the most important part...

odz: _Practice what you preach_

If you want to reproduce, add-on, falsify or apply your own ideas to this example, you can find the code (and data) in [Github.com](https://github.com/uashogeschoolutrecht/work_flows)

**In webinar 3, I will show you how to actually run, use and organize code like this!**

```{r, dpi = 60}
knitr::include_graphics(
  file.path(image_dir,
  "git_collaboration.png")
)
```

## Thank you for your attention!

```{r, dpi = 150}
knitr::include_graphics(
  file.path(
    image_dir,
    "Pepper.png")
)
```

**UPCOMING WEBINARS:**

 - Part 2; Managing your project files and data with 'Guerilla Analytics' 
 (~June 23rd, 2020)
 - Part 3; Reproducible (Open) Science @HU - Tools (~July 6th, 2020)
 
 [Peer Support Group Data Science](tln.hu.nl)
 [support voor onderzoek](https://bibliotheek.hu.nl/onderzoekers/)

## Example; The Open Science Framework [OSF](https://osf.io/)
```{r}
knitr::include_graphics(
  here::here(
    "images",
    "cos-shield.png")
)
```

$Reproducible\ Science = P + D + C + OAcc + OSrc$ 

**OSF has it all**

<p style="font-size:14px">$P = Publication$, $D = Data$, $C = Code$, $OAcc = Open\ Access$, $OSrc = Open\ Source$ </p> 

## OSF - Reproducible Project: Psychology

 >- 100 publications in Psychology journals
 >- Results from half of these publications could be reproduced
 >- Full access to P, D and C in [OSF](https://osf.io/ezcuj/)
 >- The publication is not published in an OAcc journal but:
 >- [The submitted manuscript is available in OSF](http://pps.sagepub.com/content/7/6/657.abstract)
 >- [The R code used is available in OSF](https://osf.io/fkmwg/)  
 
 $RP:Psychology = P + D + C + OSrc\ (+ OAcc)$

<p style="font-size:14px">$P = Publication$, $D = Data$, $C = Code$, $OAcc = Open\ Access$, $OSrc = Open\ Source$ </p> 

## <mark>ASSIGNMENT; "Assing a reproducibility score for scientific literature"</mark> {-}

resource: https://www.researchgate.net/publication/340244621_Reproducibility_and_reporting_practices_in_COVID-19_preprint_manuscripts/fulltext/5e81f9fd92851caef4ae37ba/Reproducibility-and-reporting-practices-in-COVID-19-preprint-manuscripts.pdf

This exercise is about identifying reproducibility issues in a scientific publication. We use the criteria for reproduciblity that are publically available [via:](https://www.researchgate.net/deref/https%3A%2F%2Fwww.ripeta.com%2Fuploads%2F7%2F4%2F8%2F7%2F7487334%2Fripeta_approach_and_criteria_definitions.pdf)

| Transparency Criteria| Definition       | Response Type|
|---------|-------------------------------|----------|
|Study Purpose |A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective.| Binary| 
|Data Availability Statement | A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section.| Binary|
|Data Location | Where the article’s data can be accessed, either raw or processed.| Found Value|
|Study Location| Author has stated in the methods section where the study took place or the data’s country/region of origin.| Binary; Found Value|
|Author Review| The professionalism of the contact information that the author has provided in the manuscript.|Found Value|
|Ethics Statement | A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data.|Binary|
|Funding Statement| A statement within the manuscript indicating whether or not the authors received funding for their research.|Binary|
Code Availability | Authors have shared access to the most updated code that they used in their study, including code used for analysis. |Binary|

**Table clarification** The `Transparency Criteria` are criteria you need to score the article of your choice for. Read them carefully and discuss with another course participant if you do not understand them. 
Each Tranparance criterion comes with a `Definition` that explains the criterion in more details. These descriptions are particularly helpful to understand what the criterium entails and what to look for in the article. The `Response Type` is the actual score  

In this assisgment you need to find a scientific article yourself, using PubMed or another database or repository. Use only Open Access articles. Having an article in hand, go over the table above and score the article according the criteria. **<mark>Be sure to select a primary article that presents a study using data from a experimental work </mark>**. This can be laboratory experiments or _in silico_ experiments. Reviews and meta analysis are not suitable for this assignment     

To guide your search you can choose between these topics

 - "Coronavirus / COVID-19"
 - "The effects of compound on an organism / Toxicology"
 - "The effectiveness of a drug or treatment in an animal study"
 - "The effects of a compound investigated in a cell or organoid system"
 
**TIPS**

 - If you do not know where to start your literature search start here: https://www.biorxiv.org/
 - This assignment is not about the topic you select, so try to do that quickly
 - You may want to cheat and select an article that scores TRUE on the `Data Availability Statement`, beacause that enables you to use the this article again in one of the next assignments.
 

To complete the assignment, execute activity A to G  
 
 - A) Initiate an empty RMarkdown file in your RStudio enviroment and provide author and title (after the title of this exercise)
 - B) Search for a primary Open Access article on one of the above listed topics, using Pubmed Central
 - C) Read the article diagonally to check if is indeed a primary article describing emperical scientific findings. 
 - D) Include the reference to this article in your Rmd file
 - E) Score the article on the basis of the above 'Repita' criteria
 - F) Write an Rmarkdown report on your findings, including the table above and some information about the article such as general aim, short methods and results. If data is available, try including some  
 - G) Store the source Rmd and knitted HTML in a folder called 'Rmd' in your course RStudio project. You will need it again later in the course
 
## <mark>Assignment</mark>

To complete this assignment you will have to follow steps A-

 - A) Using the [OSF website](https://osf.io/), select a project that addresses an aspect of the SARS-Cov-2 virus. 
 - B) Select a project that has a dataset and R-code shared in the project environment. 
 - C) Have a look at the code. Describe in your own words what the code intents to achieve.
 - D) In terms of readibility of the code, how would you grade (1(very bad)-5(very good)) the code available.
 - E) Download the code and the data to a new RStudio project
 - F) Run the script or code that is available to reproduce at least 1 figure
 - G) When you encounter errors or flaws in the script, try fixing them and record your changes.
 - H) Taken together on a scale from 1 (very hard) to 5 (very easy), how much effort did it take you to reproduce the visualization from the project, report or article
 - I) Generate an RMarkdown script that contains the details on the project you selected, the code you used to create the visualization and your score for reproducibility.
 
 
 
